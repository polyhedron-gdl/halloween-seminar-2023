{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e49b0b",
   "metadata": {
    "id": "13e49b0b"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/polyhedron-gdl/halloween-seminar-2023/blob/main/1-notebooks/chapter-10-01.ipynb\">\n",
    "        <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac51554",
   "metadata": {
    "id": "6ac51554"
   },
   "source": [
    "# Introduction to Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd85a26",
   "metadata": {
    "id": "6cd85a26"
   },
   "source": [
    "## What is Huggin Face?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e24e66",
   "metadata": {
    "id": "44e24e66"
   },
   "source": [
    "Hugging Face is an AI company that has become a major hub for open-source machine learning (ML). Their platform has 3 major elements which allow users to access and share machine learning resources.\n",
    "\n",
    "- First is their rapidly growing repository of pre-trained open-source ML models for things such as natural language processing (NLP), computer vision, and more.\n",
    "\n",
    "- Second is their library of datasets for training ML models for almost any task.\n",
    "\n",
    "- Third, and finally, is Spaces which is a collection of open-source ML apps hosted by Hugging Face.\n",
    "\n",
    "The power of these resources is that they are community generated, which leverages all the benefits of open-source (i.e. cost-free, wide diversity of tools, high-quality resources, and rapid pace of innovation). While these make building powerful ML projects more accessible than before, there is another key element of the Hugging Face ecosystem — the Transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a38a1d6",
   "metadata": {
    "id": "7a38a1d6"
   },
   "source": [
    "## Sentiment Analysis with Hugging Face Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd680ae",
   "metadata": {
    "id": "7bd680ae"
   },
   "source": [
    "In this notebook, you’ll learn how to leverage pre-trained machine learning models from Hugging Face to perform sentiment analysis on various text examples. We’ll walk you through the entire process, from installing the required packages to running and interpreting the model’s output. By the end of this tutorial, you’ll be equipped with the knowledge to use Hugging Face Transformers as a Library for analyzing the sentiment of text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107eb5e3",
   "metadata": {
    "id": "107eb5e3"
   },
   "source": [
    "**Step 1: Install Required Packages**\n",
    "\n",
    "First, you’ll need to install the transformers library from Hugging Face. You can do this using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c350f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "840c350f",
    "outputId": "671ca7a0-30ba-4cbf-f737-9a7037ea02c9"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3608330d",
   "metadata": {
    "id": "3608330d"
   },
   "source": [
    "**Transformers** is a Python library that makes downloading and training state-of-the-art ML models easy. Although it was initially made for developing language models, its functionality has expanded to include models for computer vision, audio processing, and beyond.\n",
    "\n",
    "Two big strengths of this library are:\n",
    "\n",
    "- it easily integrates with Hugging Face’s (previously mentioned) Models, Datasets, and Spaces repositories, and ...\n",
    "\n",
    "- the library supports other popular ML frameworks such as PyTorch and TensorFlow.\n",
    "\n",
    "This results in a simple and flexible all-in-one platform for downloading, training, and deploying machine learning models and apps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a246c1a4",
   "metadata": {
    "id": "a246c1a4"
   },
   "source": [
    "In this example we are going to use PyTorch as the predefined framework. You can install PyTorch by running the following command in your SingleStore Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad06cbf",
   "metadata": {
    "id": "aad06cbf"
   },
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44410152",
   "metadata": {
    "id": "44410152"
   },
   "source": [
    "> **Restart the Kernel**: After installing, you may need to restart the SingleStore Notebook kernel to ensure that the newly installed packages are recognized. You can usually do this by clicking on “Kernel” in the menu and then selecting “Restart Kernel”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014271c",
   "metadata": {
    "id": "c014271c"
   },
   "source": [
    "**Step 2: Import Libraries**\n",
    "\n",
    "Import the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85c5eb",
   "metadata": {
    "id": "8a85c5eb"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b447a2b",
   "metadata": {
    "id": "3b447a2b"
   },
   "source": [
    "> **AutoModels** In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the from_pretrained method. AutoClasses are here to do this job for you so that you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary: Instantiating one of *AutoModel*, *AutoConfig* and *AutoTokenizer* will directly create a class of the relevant architecture\n",
    "\n",
    "> **Example** `model = AutoModel.from_pretrained('bert-base-cased')` will create a instance of `BertModel`). In particular, `AutoModelForSequenceClassification` is a generic model class that will be instantiated as one of the sequence classification model classes of the library when created with the `AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path)` class method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af05dd9a",
   "metadata": {
    "id": "af05dd9a"
   },
   "source": [
    "**Step 3: Load Pre-trained Model and Tokenizer**\n",
    "\n",
    "An important remark on Hugging Face terminology\n",
    "\n",
    "> When we work with Hugging Face Libraries we must remember that the term **architecture** refers to the skeleton of the model and **checkpoints** are the weights for a given architecture. For example, **BERT is an architecture**, while **bert-base-uncased is a checkpoint**. Model is a general term that can mean either architecture or checkpoint.\n",
    "\n",
    "With so many different Transformer architectures, it can be challenging to create one for your checkpoint. As we have already noted above, as a part of Hugging Face Transformers core philosophy to make the library easy, simple and flexible to use, an `AutoClass` automatically infers and loads the correct architecture from a given checkpoint. The `from_pretrained()` method lets you quickly load a pretrained model for any architecture so you don’t have to devote time and resources to train a model from scratch. Producing this type of checkpoint-agnostic code means if your code works for one checkpoint, it will work with another checkpoint - as long as it was trained for a similar task - even if the architecture is different.\n",
    "\n",
    "For this example, let’s use the **distilbert-base-uncased-finetuned-sst-2-english** model for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde33450",
   "metadata": {
    "id": "dde33450"
   },
   "outputs": [],
   "source": [
    "# -----> HERE you define the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "# -----> HERE you define the model that will be used for the inference phase\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c13f96",
   "metadata": {
    "id": "a1c13f96"
   },
   "source": [
    " **Step 4: Preprocess Text**\n",
    "\n",
    "Tokenize the text you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c85e76",
   "metadata": {
    "id": "e5c85e76"
   },
   "outputs": [],
   "source": [
    "text = '''\n",
    "Katia Winter star in this understated, psychological thriller about a British operative who ends up having\n",
    "to confiscate a negative from a freelance photographer of a picture he took of her in order to remain as a\n",
    "ghost from those who are seeking to kill her. In the process, both she and the reluctant photographer find\n",
    "themselves on the run. Directed by Joshua Caldwell who also directed Be Somebody (2016), a comedy romance\n",
    "drama that also happens to echo a similar understated tone and offers a refreshing diversion into a more\n",
    "realistic interplay between characters that Negative presents. Negative together with Jennifer Lawrence in\n",
    "Red Sparrow (2018) and Daryl Hannah in The Job (2003) allow female assassin or professional agents that\n",
    "emphasize the psychological drama over the intensity of special effects, explosions, and action scenes of\n",
    "over the top mass killings or hand to hand combat in a choreographed martial arts. At the same time, the\n",
    "sustainability of audience's interest is made much more difficult, especially with the marketing of Negative's\n",
    "trailer with emphasized the action portion of the movie much to the disappointment of a good segment of the audience.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3609cd3",
   "metadata": {
    "id": "a3609cd3"
   },
   "outputs": [],
   "source": [
    "text = '''\n",
    "The Burial is a captivating film that tells the inspiring story of a small business owner's fight against a\n",
    "corporate giant, highlighting themes of justice and greed. The film seamlessly blends humor, heartwarming\n",
    "family moments, and intense courtroom drama, making it a compelling watch for all audiences. Jamie Foxx delivers\n",
    "a standout performance, showcasing his range and chemistry with co-star Tommy Lee Jones, while the supporting\n",
    "cast also shines in their scene-stealing roles.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e6561",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a7e6561",
    "outputId": "d91eecc2-edf1-49dd-e14b-50a81a01481f"
   },
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ee9e4e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22ee9e4e",
    "outputId": "c7683356-81fc-4fb5-97ba-0df6d3ea199b"
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3947450d",
   "metadata": {
    "id": "3947450d"
   },
   "source": [
    "**Step 5: Model Inference**\n",
    "\n",
    "Pass the tokenized text through the model. You can find a complete and clear explanation of **context manager** in python and their use [here](https://realpython.com/python-with-statement/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f1886a",
   "metadata": {
    "id": "56f1886a"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs       = model(**tokens)\n",
    "    logits        = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d943aea",
   "metadata": {
    "id": "5d943aea"
   },
   "source": [
    "Let's explain the previous code step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc492e",
   "metadata": {
    "id": "7fbc492e"
   },
   "source": [
    "1. `with torch.no_grad():`\n",
    "   - This line of code is starting a context manager using the `with` statement. The purpose of this context manager is to temporarily disable gradient computation in PyTorch. In deep learning, when you train a neural network, you typically compute gradients for the model's parameters during the forward and backward passes to perform optimization (e.g., gradient descent). However, in some cases, you might want to perform inference or evaluation without computing gradients because they are not needed for these tasks. This context manager ensures that gradients are not computed within the indented block of code that follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc921db",
   "metadata": {
    "id": "3cc921db"
   },
   "source": [
    "2. `outputs = model(**tokens):`\n",
    "   - Here, the code is making a forward pass through a neural network model. The `model` is the PyTorch model previously defined, and it's being called with `**tokens` as its argument. The `**tokens` syntax indicates that the `tokens` variable is a dictionary containing keyword arguments for the model. This line of code computes the model's output based on the input tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0444486",
   "metadata": {
    "id": "d0444486"
   },
   "source": [
    "3. `logits = outputs.logits:`\n",
    "   - `outputs` is an object returned by the model, the code is extracting the `logits` from the model's output. Logits are raw values generated by the model before applying a softmax function. These logits are often used in classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749bce22",
   "metadata": {
    "id": "749bce22"
   },
   "source": [
    "4. `probabilities = torch.softmax(logits, dim=1):`\n",
    "   - In this line, the code is taking the `logits` obtained from the model and applying the softmax function to them. The `torch.softmax` function is used to convert raw logits into probabilities. The `dim=1` argument specifies that the softmax operation should be performed along dimension 1 of the `logits` tensor. This typically corresponds to the class dimension in a classification problem, and it ensures that the resulting probabilities sum to 1 along that dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61110fad",
   "metadata": {
    "id": "61110fad"
   },
   "source": [
    "In summary, the code is performing a forward pass through a PyTorch neural network model, extracting the raw logits from the model's output, and then converting these logits into probability values using the softmax function. The use of `torch.no_grad()` ensures that gradient computation is turned off during this process, which is typically done during inference or evaluation to save computation resources and memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ce970",
   "metadata": {
    "id": "ae2ce970"
   },
   "source": [
    "**Step 6: Interpret Results**\n",
    "\n",
    "Interpret the model’s output to get the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2dd283",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc2dd283",
    "outputId": "fea1b10a-3e30-4c81-fe9d-27f430735880"
   },
   "outputs": [],
   "source": [
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c148386e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c148386e",
    "outputId": "78f95ae7-b11d-4e18-dda7-413fa7ba7de0"
   },
   "outputs": [],
   "source": [
    "label_ids = torch.argmax(probabilities, dim=1)\n",
    "labels = ['Negative', 'Positive']\n",
    "label = labels[label_ids]\n",
    "print(f\"The sentiment is: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109236af",
   "metadata": {
    "id": "109236af"
   },
   "source": [
    " Let's break this code down step by step:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94fe194",
   "metadata": {
    "id": "d94fe194"
   },
   "source": [
    "1. `label_ids = torch.argmax(probabilities, dim=1):`\n",
    "   - In this line of code, the `torch.argmax` function is used to find the index of the maximum probability along dimension 1 of the `probabilities` tensor. This essentially determines the predicted class (sentiment label) for each input. The `dim=1` argument specifies that the operation should be performed along the second dimension of the `probabilities` tensor, which is often the dimension corresponding to classes in classification problems.\n",
    "\n",
    "2. `labels = ['Negative', 'Positive']`\n",
    "   - Here, a list `labels` is defined, which contains two sentiment labels: 'Negative' and 'Positive'. These labels likely correspond to the two possible classes the model is trying to classify.\n",
    "\n",
    "3. `label = labels[label_ids]:`\n",
    "   - This line uses the `label_ids` obtained in step 1 to index the `labels` list. It essentially maps the predicted class (determined by the maximum probability) to the corresponding sentiment label. For example, if `label_ids` is `[1, 0, 1]`, it means the model predicts 'Positive', 'Negative', 'Positive' sentiments, respectively.\n",
    "\n",
    "4. `print(f\"The sentiment is: {label}\")`\n",
    "   - Finally, this line prints out the predicted sentiment label. It uses an f-string to format the output string, where `{label}` is replaced with the actual sentiment label determined in step 3.\n",
    "\n",
    "So, the overall purpose of this code snippet is to take the probabilities predicted by the model for different sentiment classes (typically 'Negative' and 'Positive') and select the sentiment label with the highest probability as the predicted sentiment for a given input. It then prints this predicted sentiment label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe77d1",
   "metadata": {
    "id": "42fe77d1"
   },
   "source": [
    "### Using Hugging Face Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AJUJnSf-uLYb",
   "metadata": {
    "id": "AJUJnSf-uLYb"
   },
   "source": [
    "The easiest way to start using the library is via the pipeline() function, which abstracts NLP (and other) tasks into 1 line of code. For example, if we want to do sentiment analysis, we would need to select a model, tokenize the input text, pass it through the model, and decode the numerical output to determine the sentiment label (positive or negative).\n",
    "\n",
    "While this may seem like a lot of steps, we can do all this in 1 line via the pipeline() function, as shown in the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "620f38af",
   "metadata": {
    "id": "620f38af"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-wJ6XzaQt-nw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wJ6XzaQt-nw",
    "outputId": "3012f76c-54b2-472f-b1bd-963588871623"
   },
   "outputs": [],
   "source": [
    "classifier = pipeline(task=\"sentiment-analysis\", \\\n",
    "                      model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3ce00",
   "metadata": {
    "id": "3db3ce00"
   },
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac1293",
   "metadata": {
    "id": "cbac1293"
   },
   "source": [
    "Another way we can use the pipeline() function is for text summarization. Although this is an entirely different task than sentiment analysis, the syntax is almost identical.\n",
    "\n",
    "We first load in a summarization model. Then pass in some text along with a couple of input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ba7ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "327ba7ac",
    "outputId": "91fdee16-5f46-4a43-d930-f684a9933487"
   },
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "text = \"\"\"\n",
    "Hugging Face is an AI company that has become a major hub for open-source machine learning.\n",
    "Their platform has 3 major elements which allow users to access and share machine learning resources.\n",
    "First, is their rapidly growing repository of pre-trained open-source machine learning models for things such as natural language processing (NLP), computer vision, and more.\n",
    "Second, is their library of datasets for training machine learning models for almost any task.\n",
    "Third, and finally, is Spaces which is a collection of open-source ML apps.\n",
    "\n",
    "The power of these resources is that they are community generated, which leverages all the benefits of open source i.e. cost-free, wide diversity of tools, high quality resources, and rapid pace of innovation.\n",
    "While these make building powerful ML projects more accessible than before, there is another key element of the Hugging Face ecosystem—their Transformers library.\n",
    "\"\"\"\n",
    "summarized_text = summarizer(text, min_length=5, max_length=100)[0]['summary_text']\n",
    "print('\\n\\n' + summarized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kGZ5Mz7ZxBcB",
   "metadata": {
    "id": "kGZ5Mz7ZxBcB"
   },
   "source": [
    "## Conversational"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94248bbb",
   "metadata": {},
   "source": [
    "Finally, we can use models developed specifically to generate conversational text. Since conversations require past prompts and responses to be passed to subsequent model responses, the syntax is a little different here. However, we start by instantiating our model using the pipeline() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c5703f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453e8253ece04e139f432e828df53a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.blenderbot.modeling_tf_blenderbot because of the following error (look up to see its traceback):\nNo module named 'keras.engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1146\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\blenderbot\\modeling_tf_blenderbot.py:34\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Public API\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     DUMMY_INPUTS,\n\u001b[0;32m     36\u001b[0m     TFCausalLanguageModelingLoss,\n\u001b[0;32m     37\u001b[0m     TFPreTrainedModel,\n\u001b[0;32m     38\u001b[0m     keras_serializable,\n\u001b[0;32m     39\u001b[0m     unpack_inputs,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shape_list, stable_softmax\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py:69\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_adapter\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasTensor\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.engine'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chatbot \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/blenderbot-400M-distill\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\__init__.py:779\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;66;03m# Infer the framework from the model\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;66;03m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;66;03m# Will load the correct model if possible\u001b[39;00m\n\u001b[0;32m    778\u001b[0m model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 779\u001b[0m framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    780\u001b[0m     model,\n\u001b[0;32m    781\u001b[0m     model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    782\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    783\u001b[0m     framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m    784\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    787\u001b[0m )\n\u001b[0;32m    789\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    790\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:238\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m         classes\u001b[38;5;241m.\u001b[39mappend(_class)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m look_tf:\n\u001b[1;32m--> 238\u001b[0m     _class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTF\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marchitecture\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m         classes\u001b[38;5;241m.\u001b[39mappend(_class)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1137\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1136\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1137\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1136\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1134\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1136\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1137\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1148\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1150\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1151\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.blenderbot.modeling_tf_blenderbot because of the following error (look up to see its traceback):\nNo module named 'keras.engine'"
     ]
    }
   ],
   "source": [
    "chatbot = pipeline(model=\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c83921",
   "metadata": {},
   "source": [
    "Next, we can use the Conversation() class to handle the back-and-forth. We initialize it with a user prompt, then pass it into the chatbot model from the previous code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c51e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Conversation\n",
    "\n",
    "conversation = Conversation(\"Hi I'm Shaw, how are you?\")\n",
    "conversation = chatbot(conversation)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c42665",
   "metadata": {},
   "source": [
    "To keep the conversation going, we can use the add_user_input() method to add another prompt to the conversation. We then pass the conversation object back into the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bf6622",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.add_user_input(\"Where do you work?\")\n",
    "conversation = chatbot(conversation)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd887a",
   "metadata": {},
   "source": [
    "### Chatbot UI with Gradio\n",
    "\n",
    "While we get the base chatbot functionality with the Transformer library, this is an inconvenient way to interact with a chatbot. To make the interaction a bit more intuitive, we can use Gradio to spin up a front end in a few lines of Python code.\n",
    "\n",
    "This is done with the code shown below. At the top, we initialize two lists to store user messages and model responses, respectively. Then we define a function that will take the user prompt and generate a chatbot output. Next, we create the chat UI using the Gradio ChatInterface() class. Finally, we launch the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "336ad4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44893296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_list = []\n",
    "response_list = []\n",
    "\n",
    "def vanilla_chatbot(message, history):\n",
    "    conversation = Conversation(text=message, past_user_inputs=message_list, generated_responses=response_list)\n",
    "    conversation = chatbot(conversation)\n",
    "\n",
    "    return conversation.generated_responses[-1]\n",
    "\n",
    "demo_chatbot = gr.ChatInterface(vanilla_chatbot, title=\"Vanilla Chatbot\", description=\"Enter text to start chatting.\")\n",
    "\n",
    "demo_chatbot.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae43511",
   "metadata": {
    "id": "aae43511"
   },
   "source": [
    "## References and Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014d9c5",
   "metadata": {
    "id": "a014d9c5"
   },
   "source": [
    "- Pavan Belegatti, \"[Hugging Face Tutorial for Beginners!](https://levelup.gitconnected.com/hugging-face-tutorial-for-beginners-e3a1c770cf9b)\"\n",
    "\n",
    "- Shawhin Talebi, \"[Cracking Open the Hugging Face Transformers Library](https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)\"\n",
    "\n",
    "- Shashank Mohan Jain, \"Introduction to Transformers for NLP\", Apress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ca57b5",
   "metadata": {
    "id": "38ca57b5"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": "10",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
