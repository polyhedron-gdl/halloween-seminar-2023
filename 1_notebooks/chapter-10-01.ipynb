{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13e49b0b",
      "metadata": {
        "id": "13e49b0b"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/polyhedron-gdl/halloween-seminar-2023/blob/main/1-notebooks/chapter-10-01.ipynb\">\n",
        "        <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ac51554",
      "metadata": {
        "id": "6ac51554"
      },
      "source": [
        "# Introduction to Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cd85a26",
      "metadata": {
        "id": "6cd85a26"
      },
      "source": [
        "## What is Huggin Face?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44e24e66",
      "metadata": {
        "id": "44e24e66"
      },
      "source": [
        "Hugging Face is an AI company that has become a major hub for open-source machine learning (ML). Their platform has 3 major elements which allow users to access and share machine learning resources.\n",
        "\n",
        "- First is their rapidly growing repository of pre-trained open-source ML models for things such as natural language processing (NLP), computer vision, and more.\n",
        "\n",
        "- Second is their library of datasets for training ML models for almost any task.\n",
        "\n",
        "- Third, and finally, is Spaces which is a collection of open-source ML apps hosted by Hugging Face.\n",
        "\n",
        "The power of these resources is that they are community generated, which leverages all the benefits of open-source (i.e. cost-free, wide diversity of tools, high-quality resources, and rapid pace of innovation). While these make building powerful ML projects more accessible than before, there is another key element of the Hugging Face ecosystem — the Transformers library."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a38a1d6",
      "metadata": {
        "id": "7a38a1d6"
      },
      "source": [
        "## Sentiment Analysis with Hugging Face Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bd680ae",
      "metadata": {
        "id": "7bd680ae"
      },
      "source": [
        "In this notebook, you’ll learn how to leverage pre-trained machine learning models from Hugging Face to perform sentiment analysis on various text examples. We’ll walk you through the entire process, from installing the required packages to running and interpreting the model’s output. By the end of this tutorial, you’ll be equipped with the knowledge to use Hugging Face Transformers as a Library for analyzing the sentiment of text data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "107eb5e3",
      "metadata": {
        "id": "107eb5e3"
      },
      "source": [
        "**Step 1: Install Required Packages**\n",
        "\n",
        "First, you’ll need to install the transformers library from Hugging Face. You can do this using pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "840c350f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "840c350f",
        "outputId": "671ca7a0-30ba-4cbf-f737-9a7037ea02c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3608330d",
      "metadata": {
        "id": "3608330d"
      },
      "source": [
        "**Transformers** is a Python library that makes downloading and training state-of-the-art ML models easy. Although it was initially made for developing language models, its functionality has expanded to include models for computer vision, audio processing, and beyond.\n",
        "\n",
        "Two big strengths of this library are:\n",
        "\n",
        "- it easily integrates with Hugging Face’s (previously mentioned) Models, Datasets, and Spaces repositories, and ...\n",
        "\n",
        "- the library supports other popular ML frameworks such as PyTorch and TensorFlow.\n",
        "\n",
        "This results in a simple and flexible all-in-one platform for downloading, training, and deploying machine learning models and apps."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a246c1a4",
      "metadata": {
        "id": "a246c1a4"
      },
      "source": [
        "In this example we are going to use PyTorch as the predefined framework. You can install PyTorch by running the following command in your SingleStore Notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "aad06cbf",
      "metadata": {
        "id": "aad06cbf"
      },
      "outputs": [],
      "source": [
        "# !pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44410152",
      "metadata": {
        "id": "44410152"
      },
      "source": [
        "> **Restart the Kernel**: After installing, you may need to restart the SingleStore Notebook kernel to ensure that the newly installed packages are recognized. You can usually do this by clicking on “Kernel” in the menu and then selecting “Restart Kernel”."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c014271c",
      "metadata": {
        "id": "c014271c"
      },
      "source": [
        "**Step 2: Import Libraries**\n",
        "\n",
        "Import the necessary Python libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8a85c5eb",
      "metadata": {
        "id": "8a85c5eb"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b447a2b",
      "metadata": {
        "id": "3b447a2b"
      },
      "source": [
        "> **AutoModels** In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the from_pretrained method. AutoClasses are here to do this job for you so that you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary: Instantiating one of *AutoModel*, *AutoConfig* and *AutoTokenizer* will directly create a class of the relevant architecture\n",
        "\n",
        "> **Example** `model = AutoModel.from_pretrained('bert-base-cased')` will create a instance of `BertModel`). In particular, `AutoModelForSequenceClassification` is a generic model class that will be instantiated as one of the sequence classification model classes of the library when created with the `AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path)` class method."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af05dd9a",
      "metadata": {
        "id": "af05dd9a"
      },
      "source": [
        "**Step 3: Load Pre-trained Model and Tokenizer**\n",
        "\n",
        "An important remark on Hugging Face terminology\n",
        "\n",
        "> When we work with Hugging Face Libraries we must remember that the term **architecture** refers to the skeleton of the model and **checkpoints** are the weights for a given architecture. For example, **BERT is an architecture**, while **bert-base-uncased is a checkpoint**. Model is a general term that can mean either architecture or checkpoint.\n",
        "\n",
        "With so many different Transformer architectures, it can be challenging to create one for your checkpoint. As we have already noted above, as a part of Hugging Face Transformers core philosophy to make the library easy, simple and flexible to use, an `AutoClass` automatically infers and loads the correct architecture from a given checkpoint. The `from_pretrained()` method lets you quickly load a pretrained model for any architecture so you don’t have to devote time and resources to train a model from scratch. Producing this type of checkpoint-agnostic code means if your code works for one checkpoint, it will work with another checkpoint - as long as it was trained for a similar task - even if the architecture is different.\n",
        "\n",
        "For this example, let’s use the **distilbert-base-uncased-finetuned-sst-2-english** model for sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "dde33450",
      "metadata": {
        "id": "dde33450"
      },
      "outputs": [],
      "source": [
        "# -----> HERE you define the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "# -----> HERE you define the model that will be used for the inference phase\n",
        "model     = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c13f96",
      "metadata": {
        "id": "a1c13f96"
      },
      "source": [
        " **Step 4: Preprocess Text**\n",
        "\n",
        "Tokenize the text you want to analyze."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e5c85e76",
      "metadata": {
        "id": "e5c85e76"
      },
      "outputs": [],
      "source": [
        "text = '''\n",
        "Katia Winter star in this understated, psychological thriller about a British operative who ends up having\n",
        "to confiscate a negative from a freelance photographer of a picture he took of her in order to remain as a\n",
        "ghost from those who are seeking to kill her. In the process, both she and the reluctant photographer find\n",
        "themselves on the run. Directed by Joshua Caldwell who also directed Be Somebody (2016), a comedy romance\n",
        "drama that also happens to echo a similar understated tone and offers a refreshing diversion into a more\n",
        "realistic interplay between characters that Negative presents. Negative together with Jennifer Lawrence in\n",
        "Red Sparrow (2018) and Daryl Hannah in The Job (2003) allow female assassin or professional agents that\n",
        "emphasize the psychological drama over the intensity of special effects, explosions, and action scenes of\n",
        "over the top mass killings or hand to hand combat in a choreographed martial arts. At the same time, the\n",
        "sustainability of audience's interest is made much more difficult, especially with the marketing of Negative's\n",
        "trailer with emphasized the action portion of the movie much to the disappointment of a good segment of the audience.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a3609cd3",
      "metadata": {
        "id": "a3609cd3"
      },
      "outputs": [],
      "source": [
        "text = '''\n",
        "The Burial is a captivating film that tells the inspiring story of a small business owner's fight against a\n",
        "corporate giant, highlighting themes of justice and greed. The film seamlessly blends humor, heartwarming\n",
        "family moments, and intense courtroom drama, making it a compelling watch for all audiences. Jamie Foxx delivers\n",
        "a standout performance, showcasing his range and chemistry with co-star Tommy Lee Jones, while the supporting\n",
        "cast also shines in their scene-stealing roles.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "9a7e6561",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a7e6561",
        "outputId": "d91eecc2-edf1-49dd-e14b-50a81a01481f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The Burial is a captivating film that tells the inspiring story of a small business owner's fight against a\n",
            "corporate giant, highlighting themes of justice and greed. The film seamlessly blends humor, heartwarming\n",
            "family moments, and intense courtroom drama, making it a compelling watch for all audiences. Jamie Foxx delivers\n",
            "a standout performance, showcasing his range and chemistry with co-star Tommy Lee Jones, while the supporting\n",
            "cast also shines in their scene-stealing roles.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "22ee9e4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22ee9e4e",
        "outputId": "c7683356-81fc-4fb5-97ba-0df6d3ea199b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1996,  8940,  2003,  1037, 14408, 17441,  2143,  2008,  4136,\n",
              "          1996, 18988,  2466,  1997,  1037,  2235,  2449,  3954,  1005,  1055,\n",
              "          2954,  2114,  1037,  5971,  5016,  1010, 20655,  6991,  1997,  3425,\n",
              "          1998, 22040,  1012,  1996,  2143, 25180, 10895, 12586,  2015,  8562,\n",
              "          1010,  2540,  9028,  6562,  2155,  5312,  1010,  1998,  6387, 20747,\n",
              "          3689,  1010,  2437,  2009,  1037, 17075,  3422,  2005,  2035,  9501,\n",
              "          1012,  6175,  4419,  2595, 18058,  1037,  3233,  5833,  2836,  1010,\n",
              "         27696,  2010,  2846,  1998,  6370,  2007,  2522,  1011,  2732,  6838,\n",
              "          3389,  3557,  1010,  2096,  1996,  4637,  3459,  2036, 12342,  2015,\n",
              "          1999,  2037,  3496,  1011, 11065,  4395,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3947450d",
      "metadata": {
        "id": "3947450d"
      },
      "source": [
        "**Step 5: Model Inference**\n",
        "\n",
        "Pass the tokenized text through the model. You can find a complete and clear explanation of **context manager** in python and their use [here](https://realpython.com/python-with-statement/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "56f1886a",
      "metadata": {
        "id": "56f1886a"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    outputs       = model(**tokens)\n",
        "    logits        = outputs.logits\n",
        "    probabilities = torch.softmax(logits, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d943aea",
      "metadata": {
        "id": "5d943aea"
      },
      "source": [
        "Let's explain the previous code step-by-step"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fbc492e",
      "metadata": {
        "id": "7fbc492e"
      },
      "source": [
        "1. `with torch.no_grad():`\n",
        "   - This line of code is starting a context manager using the `with` statement. The purpose of this context manager is to temporarily disable gradient computation in PyTorch. In deep learning, when you train a neural network, you typically compute gradients for the model's parameters during the forward and backward passes to perform optimization (e.g., gradient descent). However, in some cases, you might want to perform inference or evaluation without computing gradients because they are not needed for these tasks. This context manager ensures that gradients are not computed within the indented block of code that follows."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cc921db",
      "metadata": {
        "id": "3cc921db"
      },
      "source": [
        "2. `outputs = model(**tokens):`\n",
        "   - Here, the code is making a forward pass through a neural network model. The `model` is the PyTorch model previously defined, and it's being called with `**tokens` as its argument. The `**tokens` syntax indicates that the `tokens` variable is a dictionary containing keyword arguments for the model. This line of code computes the model's output based on the input tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0444486",
      "metadata": {
        "id": "d0444486"
      },
      "source": [
        "3. `logits = outputs.logits:`\n",
        "   - `outputs` is an object returned by the model, the code is extracting the `logits` from the model's output. Logits are raw values generated by the model before applying a softmax function. These logits are often used in classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "749bce22",
      "metadata": {
        "id": "749bce22"
      },
      "source": [
        "4. `probabilities = torch.softmax(logits, dim=1):`\n",
        "   - In this line, the code is taking the `logits` obtained from the model and applying the softmax function to them. The `torch.softmax` function is used to convert raw logits into probabilities. The `dim=1` argument specifies that the softmax operation should be performed along dimension 1 of the `logits` tensor. This typically corresponds to the class dimension in a classification problem, and it ensures that the resulting probabilities sum to 1 along that dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61110fad",
      "metadata": {
        "id": "61110fad"
      },
      "source": [
        "In summary, the code is performing a forward pass through a PyTorch neural network model, extracting the raw logits from the model's output, and then converting these logits into probability values using the softmax function. The use of `torch.no_grad()` ensures that gradient computation is turned off during this process, which is typically done during inference or evaluation to save computation resources and memory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae2ce970",
      "metadata": {
        "id": "ae2ce970"
      },
      "source": [
        "**Step 6: Interpret Results**\n",
        "\n",
        "Interpret the model’s output to get the sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "cc2dd283",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc2dd283",
        "outputId": "fea1b10a-3e30-4c81-fe9d-27f430735880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.2978e-04, 9.9987e-01]])\n"
          ]
        }
      ],
      "source": [
        "print(probabilities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "c148386e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c148386e",
        "outputId": "78f95ae7-b11d-4e18-dda7-413fa7ba7de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment is: Positive\n"
          ]
        }
      ],
      "source": [
        "label_ids = torch.argmax(probabilities, dim=1)\n",
        "labels = ['Negative', 'Positive']\n",
        "label = labels[label_ids]\n",
        "print(f\"The sentiment is: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "109236af",
      "metadata": {
        "id": "109236af"
      },
      "source": [
        " Let's break this code down step by step:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d94fe194",
      "metadata": {
        "id": "d94fe194"
      },
      "source": [
        "1. `label_ids = torch.argmax(probabilities, dim=1):`\n",
        "   - In this line of code, the `torch.argmax` function is used to find the index of the maximum probability along dimension 1 of the `probabilities` tensor. This essentially determines the predicted class (sentiment label) for each input. The `dim=1` argument specifies that the operation should be performed along the second dimension of the `probabilities` tensor, which is often the dimension corresponding to classes in classification problems.\n",
        "\n",
        "2. `labels = ['Negative', 'Positive']`\n",
        "   - Here, a list `labels` is defined, which contains two sentiment labels: 'Negative' and 'Positive'. These labels likely correspond to the two possible classes the model is trying to classify.\n",
        "\n",
        "3. `label = labels[label_ids]:`\n",
        "   - This line uses the `label_ids` obtained in step 1 to index the `labels` list. It essentially maps the predicted class (determined by the maximum probability) to the corresponding sentiment label. For example, if `label_ids` is `[1, 0, 1]`, it means the model predicts 'Positive', 'Negative', 'Positive' sentiments, respectively.\n",
        "\n",
        "4. `print(f\"The sentiment is: {label}\")`\n",
        "   - Finally, this line prints out the predicted sentiment label. It uses an f-string to format the output string, where `{label}` is replaced with the actual sentiment label determined in step 3.\n",
        "\n",
        "So, the overall purpose of this code snippet is to take the probabilities predicted by the model for different sentiment classes (typically 'Negative' and 'Positive') and select the sentiment label with the highest probability as the predicted sentiment for a given input. It then prints this predicted sentiment label."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42fe77d1",
      "metadata": {
        "id": "42fe77d1"
      },
      "source": [
        "### Using Hugging Face Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AJUJnSf-uLYb",
      "metadata": {
        "id": "AJUJnSf-uLYb"
      },
      "source": [
        "The easiest way to start using the library is via the pipeline() function, which abstracts NLP (and other) tasks into 1 line of code. For example, if we want to do sentiment analysis, we would need to select a model, tokenize the input text, pass it through the model, and decode the numerical output to determine the sentiment label (positive or negative).\n",
        "\n",
        "While this may seem like a lot of steps, we can do all this in 1 line via the pipeline() function, as shown in the code snippet below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "620f38af",
      "metadata": {
        "id": "620f38af"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "-wJ6XzaQt-nw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wJ6XzaQt-nw",
        "outputId": "3012f76c-54b2-472f-b1bd-963588871623"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998701810836792}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "classifier = pipeline(task=\"sentiment-analysis\", \\\n",
        "                      model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "classifier(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3db3ce00",
      "metadata": {
        "id": "3db3ce00"
      },
      "source": [
        "## Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbac1293",
      "metadata": {
        "id": "cbac1293"
      },
      "source": [
        "Another way we can use the pipeline() function is for text summarization. Although this is an entirely different task than sentiment analysis, the syntax is almost identical.\n",
        "\n",
        "We first load in a summarization model. Then pass in some text along with a couple of input parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "327ba7ac",
      "metadata": {
        "id": "327ba7ac",
        "outputId": "91fdee16-5f46-4a43-d930-f684a9933487",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Hugging Face is an AI company that has become a major hub for open-source machine learning. They have 3 major elements which allow users to access and share machine learning resources.\n"
          ]
        }
      ],
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "text = \"\"\"\n",
        "Hugging Face is an AI company that has become a major hub for open-source machine learning.\n",
        "Their platform has 3 major elements which allow users to access and share machine learning resources.\n",
        "First, is their rapidly growing repository of pre-trained open-source machine learning models for things such as natural language processing (NLP), computer vision, and more.\n",
        "Second, is their library of datasets for training machine learning models for almost any task.\n",
        "Third, and finally, is Spaces which is a collection of open-source ML apps.\n",
        "\n",
        "The power of these resources is that they are community generated, which leverages all the benefits of open source i.e. cost-free, wide diversity of tools, high quality resources, and rapid pace of innovation.\n",
        "While these make building powerful ML projects more accessible than before, there is another key element of the Hugging Face ecosystem—their Transformers library.\n",
        "\"\"\"\n",
        "summarized_text = summarizer(text, min_length=5, max_length=100)[0]['summary_text']\n",
        "print('\\n\\n' + summarized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kGZ5Mz7ZxBcB"
      },
      "id": "kGZ5Mz7ZxBcB"
    },
    {
      "cell_type": "markdown",
      "id": "aae43511",
      "metadata": {
        "id": "aae43511"
      },
      "source": [
        "## References and Credits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a014d9c5",
      "metadata": {
        "id": "a014d9c5"
      },
      "source": [
        "- Pavan Belegatti, \"[Hugging Face Tutorial for Beginners!](https://levelup.gitconnected.com/hugging-face-tutorial-for-beginners-e3a1c770cf9b)\"\n",
        "\n",
        "- Shawhin Talebi, \"[Cracking Open the Hugging Face Transformers Library](https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)\"\n",
        "\n",
        "- Shashank Mohan Jain, \"Introduction to Transformers for NLP\", Apress"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38ca57b5",
      "metadata": {
        "id": "38ca57b5"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": "10",
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}