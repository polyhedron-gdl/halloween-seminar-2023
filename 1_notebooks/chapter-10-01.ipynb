{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03efaeb5",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/polyhedron-gdl/halloween-seminar-2023/blob/main/1_notebooks/chapter-10-01.ipynb\">\n",
    "        <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac51554",
   "metadata": {},
   "source": [
    "# Introduction to Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c6c0f",
   "metadata": {},
   "source": [
    "## What is Huggin Face?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4151a0b6",
   "metadata": {},
   "source": [
    "Hugging Face is an AI company that has become a major hub for open-source machine learning (ML). Their platform has 3 major elements which allow users to access and share machine learning resources.\n",
    "\n",
    "- First is their rapidly growing repository of pre-trained open-source ML models for things such as natural language processing (NLP), computer vision, and more. \n",
    "\n",
    "- Second is their library of datasets for training ML models for almost any task. \n",
    "\n",
    "- Third, and finally, is Spaces which is a collection of open-source ML apps hosted by Hugging Face.\n",
    "\n",
    "The power of these resources is that they are community generated, which leverages all the benefits of open-source (i.e. cost-free, wide diversity of tools, high-quality resources, and rapid pace of innovation). While these make building powerful ML projects more accessible than before, there is another key element of the Hugging Face ecosystem — the Transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2af13b",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Hugging Face Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5b410",
   "metadata": {},
   "source": [
    "In this notebook, you’ll learn how to leverage pre-trained machine learning models from Hugging Face to perform sentiment analysis on various text examples. We’ll walk you through the entire process, from installing the required packages to running and interpreting the model’s output. By the end of this tutorial, you’ll be equipped with the knowledge to use Hugging Face Transformers as a Library for analyzing the sentiment of text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107eb5e3",
   "metadata": {},
   "source": [
    "**Step 1: Install Required Packages**\n",
    "\n",
    "First, you’ll need to install the transformers library from Hugging Face. You can do this using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964debfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b43ff6",
   "metadata": {},
   "source": [
    "**Transformers** is a Python library that makes downloading and training state-of-the-art ML models easy. Although it was initially made for developing language models, its functionality has expanded to include models for computer vision, audio processing, and beyond.\n",
    "\n",
    "Two big strengths of this library are:\n",
    "\n",
    "- it easily integrates with Hugging Face’s (previously mentioned) Models, Datasets, and Spaces repositories, and ... \n",
    "\n",
    "- the library supports other popular ML frameworks such as PyTorch and TensorFlow.\n",
    "\n",
    "This results in a simple and flexible all-in-one platform for downloading, training, and deploying machine learning models and apps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b075f4",
   "metadata": {},
   "source": [
    "In this example we are going to use PyTorch as the predefined framework. You can install PyTorch by running the following command in your SingleStore Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe4b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7189c835",
   "metadata": {},
   "source": [
    "> **Restart the Kernel**: After installing, you may need to restart the SingleStore Notebook kernel to ensure that the newly installed packages are recognized. You can usually do this by clicking on “Kernel” in the menu and then selecting “Restart Kernel”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07520d9",
   "metadata": {},
   "source": [
    "**Step 2: Import Libraries**\n",
    "\n",
    "Import the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04d84ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948847b",
   "metadata": {},
   "source": [
    "> **AutoModels** In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the from_pretrained method. AutoClasses are here to do this job for you so that you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary: Instantiating one of *AutoModel*, *AutoConfig* and *AutoTokenizer* will directly create a class of the relevant architecture\n",
    "\n",
    "> **Example** `model = AutoModel.from_pretrained('bert-base-cased')` will create a instance of `BertModel`). In particular, `AutoModelForSequenceClassification` is a generic model class that will be instantiated as one of the sequence classification model classes of the library when created with the `AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path)` class method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64602e33",
   "metadata": {},
   "source": [
    "**Step 3: Load Pre-trained Model and Tokenizer**\n",
    "\n",
    "An important remark on Hugging Face terminology\n",
    "\n",
    "> When we work with Hugging Face Libraries we must remember that the term **architecture** refers to the skeleton of the model and **checkpoints** are the weights for a given architecture. For example, **BERT is an architecture**, while **bert-base-uncased is a checkpoint**. Model is a general term that can mean either architecture or checkpoint.\n",
    "\n",
    "With so many different Transformer architectures, it can be challenging to create one for your checkpoint. As we have already noted above, as a part of Hugging Face Transformers core philosophy to make the library easy, simple and flexible to use, an `AutoClass` automatically infers and loads the correct architecture from a given checkpoint. The `from_pretrained()` method lets you quickly load a pretrained model for any architecture so you don’t have to devote time and resources to train a model from scratch. Producing this type of checkpoint-agnostic code means if your code works for one checkpoint, it will work with another checkpoint - as long as it was trained for a similar task - even if the architecture is different.\n",
    "\n",
    "For this example, let’s use the **distilbert-base-uncased-finetuned-sst-2-english** model for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc295500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----> HERE you define the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "# -----> HERE you define the model that will be used for the inference phase\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffdfd77",
   "metadata": {},
   "source": [
    " **Step 4: Preprocess Text**\n",
    "\n",
    "Tokenize the text you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2b93d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "The Burial is a captivating film that tells the inspiring story of a small business owner's fight against a \n",
    "corporate giant, highlighting themes of justice and greed. The film seamlessly blends humor, heartwarming \n",
    "family moments, and intense courtroom drama, making it a compelling watch for all audiences. Jamie Foxx delivers \n",
    "a standout performance, showcasing his range and chemistry with co-star Tommy Lee Jones, while the supporting \n",
    "cast also shines in their scene-stealing roles.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6441175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "This is a movie about how words aren't cool, but you can still expect a girl to fall at your feet in response \n",
    "to mild wordplay. Please keep up. Or throw whatever device you’re reading this on into the ocean. Send me a \n",
    "postcard ... tell me what it’s like to be free.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d013e046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is a movie about how words aren't cool, but you can still expect a girl to fall at your feet in response \n",
      "to mild wordplay. Please keep up. Or throw whatever device you’re reading this on into the ocean. Send me a \n",
      "postcard ... tell me what it’s like to be free.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8547e5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  2003,  1037,  3185,  2055,  2129,  2616,  4995,  1005,\n",
       "          1056,  4658,  1010,  2021,  2017,  2064,  2145,  5987,  1037,  2611,\n",
       "          2000,  2991,  2012,  2115,  2519,  1999,  3433,  2000, 10256,  2773,\n",
       "         13068,  1012,  3531,  2562,  2039,  1012,  2030,  5466,  3649,  5080,\n",
       "          2017,  1521,  2128,  3752,  2023,  2006,  2046,  1996,  4153,  1012,\n",
       "          4604,  2033,  1037,  2695, 11522,  1012,  1012,  1012,  2425,  2033,\n",
       "          2054,  2009,  1521,  1055,  2066,  2000,  2022,  2489,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf4b10",
   "metadata": {},
   "source": [
    "**Step 5: Model Inference**\n",
    "\n",
    "Pass the tokenized text through the model. You can find a complete and clear explanation of **context manager** in python and their use [here](https://realpython.com/python-with-statement/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4476ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs       = model(**tokens)\n",
    "    logits        = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39604d24",
   "metadata": {},
   "source": [
    "Let's explain the previous code step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800f183e",
   "metadata": {},
   "source": [
    "1. `with torch.no_grad():`\n",
    "   - This line of code is starting a context manager using the `with` statement. The purpose of this context manager is to temporarily disable gradient computation in PyTorch. In deep learning, when you train a neural network, you typically compute gradients for the model's parameters during the forward and backward passes to perform optimization (e.g., gradient descent). However, in some cases, you might want to perform inference or evaluation without computing gradients because they are not needed for these tasks. This context manager ensures that gradients are not computed within the indented block of code that follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc872b6b",
   "metadata": {},
   "source": [
    "2. `outputs = model(**tokens):`\n",
    "   - Here, the code is making a forward pass through a neural network model. The `model` is the PyTorch model previously defined, and it's being called with `**tokens` as its argument. The `**tokens` syntax indicates that the `tokens` variable is a dictionary containing keyword arguments for the model. This line of code computes the model's output based on the input tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac0ed53",
   "metadata": {},
   "source": [
    "3. `logits = outputs.logits:`\n",
    "   - `outputs` is an object returned by the model, the code is extracting the `logits` from the model's output. Logits are raw values generated by the model before applying a softmax function. These logits are often used in classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d765e62",
   "metadata": {},
   "source": [
    "4. `probabilities = torch.softmax(logits, dim=1):`\n",
    "   - In this line, the code is taking the `logits` obtained from the model and applying the softmax function to them. The `torch.softmax` function is used to convert raw logits into probabilities. The `dim=1` argument specifies that the softmax operation should be performed along dimension 1 of the `logits` tensor. This typically corresponds to the class dimension in a classification problem, and it ensures that the resulting probabilities sum to 1 along that dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c331781",
   "metadata": {},
   "source": [
    "In summary, the code is performing a forward pass through a PyTorch neural network model, extracting the raw logits from the model's output, and then converting these logits into probability values using the softmax function. The use of `torch.no_grad()` ensures that gradient computation is turned off during this process, which is typically done during inference or evaluation to save computation resources and memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec4215",
   "metadata": {},
   "source": [
    "**Step 6: Interpret Results**\n",
    "\n",
    "Interpret the model’s output to get the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a90a8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9814, 0.0186]])\n"
     ]
    }
   ],
   "source": [
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7d1c0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment is: Negative\n"
     ]
    }
   ],
   "source": [
    "label_ids = torch.argmax(probabilities, dim=1)\n",
    "labels = ['Negative', 'Positive']\n",
    "label = labels[label_ids]\n",
    "print(f\"The sentiment is: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd76120",
   "metadata": {},
   "source": [
    " Let's break this code down step by step:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b39ae1",
   "metadata": {},
   "source": [
    "1. `label_ids = torch.argmax(probabilities, dim=1):`\n",
    "   - In this line of code, the `torch.argmax` function is used to find the index of the maximum probability along dimension 1 of the `probabilities` tensor. This essentially determines the predicted class (sentiment label) for each input. The `dim=1` argument specifies that the operation should be performed along the second dimension of the `probabilities` tensor, which is often the dimension corresponding to classes in classification problems.\n",
    "\n",
    "2. `labels = ['Negative', 'Positive']`\n",
    "   - Here, a list `labels` is defined, which contains two sentiment labels: 'Negative' and 'Positive'. These labels likely correspond to the two possible classes the model is trying to classify.\n",
    "\n",
    "3. `label = labels[label_ids]:`\n",
    "   - This line uses the `label_ids` obtained in step 1 to index the `labels` list. It essentially maps the predicted class (determined by the maximum probability) to the corresponding sentiment label. For example, if `label_ids` is `[1, 0, 1]`, it means the model predicts 'Positive', 'Negative', 'Positive' sentiments, respectively.\n",
    "\n",
    "4. `print(f\"The sentiment is: {label}\")`\n",
    "   - Finally, this line prints out the predicted sentiment label. It uses an f-string to format the output string, where `{label}` is replaced with the actual sentiment label determined in step 3.\n",
    "\n",
    "So, the overall purpose of this code snippet is to take the probabilities predicted by the model for different sentiment classes (typically 'Negative' and 'Positive') and select the sentiment label with the highest probability as the predicted sentiment for a given input. It then prints this predicted sentiment label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5201117",
   "metadata": {},
   "source": [
    "## Further Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e932d38",
   "metadata": {},
   "source": [
    "For further examples see also the notebook chapter-11-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a690ad74",
   "metadata": {},
   "source": [
    "## References and Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875914c2",
   "metadata": {},
   "source": [
    "- Pavan Belegatti, \"[Hugging Face Tutorial for Beginners!](https://levelup.gitconnected.com/hugging-face-tutorial-for-beginners-e3a1c770cf9b)\"\n",
    "\n",
    "- Shawhin Talebi, \"[Cracking Open the Hugging Face Transformers Library](https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)\"\n",
    "\n",
    "- Shashank Mohan Jain, \"Introduction to Transformers for NLP\", Apress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": "10",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
