{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f54b2c72",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/polyhedron-gdl/halloween-seminar-2023/blob/main/1_notebooks/chapter-08-01.ipynb\">\n",
    "        <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df59cf9",
   "metadata": {},
   "source": [
    "# Pre-trained LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9e401",
   "metadata": {},
   "source": [
    "## What are Pre-trained Language Models?\n",
    "\n",
    "Pre-trained language models are machine learning models that have been trained on large amounts of text data and can be fine-tuned for specific natural language processing (NLP) tasks. These models learn general language features, such as grammar, syntax, and semantics, which can be adapted to various NLP tasks, such as sentiment analysis, named entity recognition, and text summarization.\n",
    "\n",
    "Examples of Pre-trained Language Models\n",
    "Some popular pre-trained language models include:\n",
    "\n",
    "- BERT (Bidirectional Encoder Representations from Transformers)\n",
    "- GPT-3 (Generative Pre-trained Transformer 3)\n",
    "- RoBERTa (Robustly optimized BERT approach)\n",
    "- T5 (Text-to-Text Transfer Transformer)\n",
    "- OpenAI Codex\n",
    "\n",
    "These pre-trained models serve as a valuable starting point for building more specialized models or solving specific tasks without the need for extensive training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c4144",
   "metadata": {},
   "source": [
    "## What is Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab7b79e",
   "metadata": {},
   "source": [
    "**Transfer learning** is a machine learning technique where a pre-trained model is adapted for a new, but similar problem. One of the key steps in transfer learning is the ability to freeze the layers of the pre-trained model so that only some portions of the network are updated during training. Freezing is crucial when you want to maintain the features that the pre-trained model has already learned.\n",
    "\n",
    "Transfer learning leverages the idea that the **knowledge learned from solving one task can be beneficial for solving a different task, even if the two tasks are not exactly the same**. By using pre-trained models as a starting point, the process of training a new model for a target task becomes more efficient and often requires less data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93406687",
   "metadata": {},
   "source": [
    "### Transfer Learning Steps\n",
    "\n",
    "- **Pre-training**: Initially, a model is trained on a large-scale dataset and a complex task, such as image recognition on a large collection of images. This pre-training step requires significant computational resources and is often performed on high-performance hardware.\n",
    "\n",
    "- **Feature Extraction**: After pre-training, the model’s parameters (weights and biases) are saved. The pre-trained model is used as a feature extractor to obtain meaningful representations or features from the data for the target task. For example, in computer vision, the model can extract high-level features like edges, textures, or shapes from images.\n",
    "\n",
    "- **Fine-tuning**: The pre-trained model’s feature extractor is combined with a new, shallow layer (or a few layers) that is specific to the target task. This new layer is randomly initialized or initialized with small weights. The entire model is then fine-tuned using the data from the target task. During fine-tuning, the new layer’s parameters are updated, while the parameters of the pre-trained layers may be updated as well or frozen to retain their knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489de2dc",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea142dd5",
   "metadata": {},
   "source": [
    "Back in 2018, Google developed a powerful Transformer-based machine learning model for NLP applications that outperforms previous language models in different benchmark datasets. And this model is called BERT.\n",
    "\n",
    "BERT is an acronym for Bidirectional Encoder Representations from Transformers. \n",
    "\n",
    "BERT architecture consists of several Transformer encoders stacked together. Each Transformer encoder encapsulates two sub-layers: a self-attention layer and a feed-forward layer.\n",
    "\n",
    "There are two different BERT models:\n",
    "\n",
    "- BERT base, which is a BERT model consists of 12 layers of Transformer encoder, 12 attention heads, 768 hidden size, and 110M parameters.\n",
    "\n",
    "- BERT large, which is a BERT model consists of 24 layers of Transformer encoder,16 attention heads, 1024 hidden size, and 340 parameters.\n",
    "\n",
    "It is pre-trained on unlabeled data mainly extracted from BooksCorpus, which has 800M words, and from Wikipedia, which has 2,500M words.\n",
    "As the name suggests, it is pre-trained by utilizing the **bidirectional** nature of the encoder stacks. This means that BERT learns information from a sequence of words not only from left to right, but also from right to left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8b5886",
   "metadata": {},
   "source": [
    "## BERT Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead9f389",
   "metadata": {},
   "source": [
    "This code uses the Hugging Face Transformers library to tokenize text using the BERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "249ef0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b74143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text = \"BERT preprocessing is essential.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaee3c6",
   "metadata": {},
   "source": [
    "### Example: Sentiment Analysis with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945520ef",
   "metadata": {},
   "source": [
    "**Distilled Models**\n",
    "\n",
    "A distilled language model, often referred to as a \"distilled model,\" is a smaller and more computationally efficient version of a larger, pre-existing language model. The process of distillation involves training a compact model to mimic the behavior and knowledge of a larger, more powerful model. The goal is to create a smaller model that retains much of the original model's performance while being more resource-efficient and faster for inference.\n",
    "\n",
    "The general steps in creating a distilled language model are as follows:\n",
    "\n",
    "1. **Teacher Model**: Start with a large, state-of-the-art language model, which is often referred to as the \"teacher model.\" This teacher model is a highly capable model with a large number of parameters and significant computational requirements.\n",
    "\n",
    "2. **Student Model**: Create a smaller model, called the \"student model,\" that has a reduced number of parameters and is more lightweight. The student model is designed to mimic the teacher model's behavior.\n",
    "\n",
    "3. **Distillation**: Train the student model using a distillation process. During training, the student model learns to predict the same or similar outputs as the teacher model. This is typically done by minimizing the difference between the student model's predictions and the teacher model's predictions on a given dataset, often with a loss function that encourages similarity in output distributions.\n",
    "\n",
    "4. **Knowledge Transfer**: The teacher model's knowledge and linguistic capabilities are effectively transferred to the student model. This includes learning patterns, language understanding, and contextual information.\n",
    "\n",
    "The benefits of distilled language models are as follows:\n",
    "\n",
    "1. **Reduced Size**: Distilled models are significantly smaller in terms of the number of parameters, making them more memory-efficient and suitable for deployment in resource-constrained environments.\n",
    "\n",
    "2. **Faster Inference**: Smaller models generally perform inference faster, which is crucial for applications where real-time or near-real-time responses are required.\n",
    "\n",
    "3. **Comparable Performance**: The goal of distillation is to ensure that the student model's performance is close to that of the teacher model. While it may not achieve the exact performance, it should still perform well on various natural language processing tasks.\n",
    "\n",
    "4. **Deployment**: Smaller, distilled models are easier to deploy on edge devices, mobile applications, and in scenarios where computational resources are limited.\n",
    "\n",
    "Distilled language models have become popular as they strike a balance between model size and performance, making them more practical for a wide range of applications. They are particularly useful in scenarios where the original, larger models are too resource-intensive to deploy. In the next example we are going to use the [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) which is tuned with the [SST-2](https://huggingface.co/datasets/sst2) dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612fb6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT tokenizer and model for sentiment classification\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc6112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input text for classification\n",
    "sentence_a = 'i love this product, it helped me alot'\n",
    "sentence_b = 'this does not work for me'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e2bcb8",
   "metadata": {},
   "source": [
    "Let’s see the tokenizer in action. We provide two sentences to the tokenizer and it returns a **dictionary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the input text\n",
    "inputs = tokenizer([sentence_a, sentence_b], return_tensors='pt', padding=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e92a20d",
   "metadata": {},
   "source": [
    "The first key is `input_ids` and its value consist of integers which are tokenized representations of the original words in the sentence. In each sequence of tokens, there are two special tokens that BERT would expect as an input:\n",
    "\n",
    "- **[CLS]**: This is the first token of every sequence, which stands for classification token.\n",
    "\n",
    "- **[SEP]**: This is the token that makes BERT know which token belongs to which sequence. This special token is mainly important for a next sentence prediction task or question-answering task. If we only have one sequence, then this token will be appended to the end of the sequence.\n",
    "\n",
    "The token `101` and `102` is the **start of sentence** and **end of sentence** token respectively. \n",
    "\n",
    "The maximum size of tokens that can be fed into BERT model is 512. If the tokens in a sequence are less than 512, we can use padding to fill the unused token slots with [PAD] token. If the tokens in a sequence are longer than 512, then we need to do a truncation.**The token 0 is the padding token** used to ensure that the length of both sequences are the same.\n",
    "\n",
    "The second key `attention_mask` is an array of binary values. Each position of the attention_mask corresponds to a token in the same position in `the input_ids`. ***1 indicates that the token at the given position should be attended to and 0 indicates that the token at the given position is a padded value***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference with the model\n",
    "# Note: In Python, the ** syntax is used for unpacking keyword arguments from a dictionary. \n",
    "# In this context, it's used to pass the inputs dictionary to the model as keyword arguments.\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310a123d",
   "metadata": {},
   "source": [
    "The instruction `torch.argmax(outputs.logits, dim=1)` is used to find the indices of the maximum values along the second dimension of the `outputs.logits` tensor. This is often used in classification tasks to determine the predicted class or label based on the model's raw scores (logits). Each row of the output tensor corresponds to a different example or data point, and the index of the maximum value in each row indicates the predicted class for that example. See [here](https://huggingface.co/transformers/v3.1.0/main_classes/output.html) for a complete description of the output structure of a PyTorch Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4201f125",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(outputs.logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(sentence_a, return_tensors='pt', padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8a1427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference with the model\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655538df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted class label\n",
    "predicted_class = torch.argmax(outputs.logits, dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d880f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class labels\n",
    "class_labels = ['Negative', 'Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e4128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the predicted sentiment\n",
    "print(f\"The sentiment of the text is: {class_labels[predicted_class]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac12da79",
   "metadata": {},
   "source": [
    "With `pipeline` it is possible to build a sentiment analysis model with just 3 lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e387f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment = pipeline(task = 'sentiment-analysis', framework='pt', model = 'distilbert-base-uncased-finetuned-sst-2-english')\n",
    "results = sentiment('i am in a very bad mood today')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e9a8e",
   "metadata": {},
   "source": [
    "## Text Classification with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424117f",
   "metadata": {},
   "source": [
    "In this section, we’re going to use the BBC News Classification dataset. If you want to follow along, you can download the dataset on [Kaggle](https://www.kaggle.com/datasets/sainijagjit/bbc-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5249005b",
   "metadata": {},
   "source": [
    "For details please refer to the article of **Ruben Winastwan** quoted in the Reference and Credit Section from which this section is taken.\n",
    "\n",
    "This dataset is already in CSV format and it has 2126 different texts, each labeled under one of 5 categories: entertainment, sport, tech, business, or politics. Let’s take a look at what the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datapath = './data/bbc-text.csv'\n",
    "df = pd.read_csv(datapath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f403405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['category']).size().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2304aba",
   "metadata": {},
   "source": [
    "As you can see, the dataframe only has two columns, which is category that will be our label, and text which will be our input data for BERT.\n",
    "\n",
    "### Preprocessing Data\n",
    "\n",
    "As you might already know from the previous section, we need to transform our text into the format that BERT expects by adding [CLS] and [SEP] tokens. We can do this easily with BertTokenizer class from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "#example_text = 'I will watch Memento tonight'\n",
    "example_text ='''\n",
    "This code essentially trains a neural network model for a specified number of epochs using training and validation datasets, \n",
    "while keeping track of accuracy and loss. The model is updated using backpropagation and the Adam optimizer during each epoch.\n",
    "'''\n",
    "bert_input = tokenizer(example_text,padding='max_length', max_length = 60, \n",
    "                       truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "print(bert_input['input_ids'])\n",
    "print(bert_input['token_type_ids'])\n",
    "print(bert_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e52ca0",
   "metadata": {},
   "source": [
    "Here is the explanation of BertTokenizer parameters above:\n",
    "\n",
    "- `padding` : to pad each sequence to the maximum length that you specify.\n",
    "- `max_length` : the maximum length of each sequence. In this example we use 10, but for our actual dataset we will use 512, which is the maximum length of a sequence allowed for BERT.\n",
    "- `truncation` : if True, then the tokens in each sequence that exceed the maximum length will be truncated.\n",
    "- `return_tensors` : the type of tensors that will be returned. Since we’re going to use **Pytorch**, then we use **pt**. If you use **Tensorflow**, then you need to use **tf**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa3d47",
   "metadata": {},
   "source": [
    "The first row is `input_ids` , which is the id representation of each token. We can actually decode these input ids into the actual tokens as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1bc9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = tokenizer.decode(bert_input.input_ids[0])\n",
    "\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90461bc9",
   "metadata": {},
   "source": [
    "As you can see, the `BertTokenizer` takes care of all of the necessary transformations of the input text such that it’s ready to be used as an input for our BERT model. It adds [CLS], [SEP], and [PAD] tokens automatically. Since we specified the maximum length to be 10, then there are only two [PAD] tokens at the end.\n",
    "\n",
    "The second row is `token_type_ids`, which is a binary mask that identifies in which sequence a token belongs. If we only have a single sequence, then all of the token type ids will be 0. For a text classification task, `token_type_ids` is an optional input for our BERT model.\n",
    "\n",
    "Finally, the third row is `attention_mask`, which is a binary mask that identifies whether a token is a real word or just padding. If the token contains [CLS], [SEP], or any real word, then the mask would be 1. Meanwhile, if the token is just padding or [PAD], then the mask would be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb261d8b",
   "metadata": {},
   "source": [
    "Let's refer to the course slides to understand why in a classification task **it is sufficient to focus on the embedding of the \n",
    "CLS token**. For further information on this topic you can usefully consult the Reference and Credits section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6a8a37",
   "metadata": {},
   "source": [
    "### Dataset Class\n",
    "\n",
    "Now that we know what kind of output that we will get from BertTokenizer , let’s build a Dataset class for our news dataset that will serve as a class to generate our news data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a8ab1",
   "metadata": {},
   "source": [
    "Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. \n",
    "\n",
    "**Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples**.\n",
    "\n",
    "For further details see the original tutorial of PyTorch [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ade1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "'''\n",
    "The class Dataset subclass the torch class Dataset. This Dataset class is designed to facilitate \n",
    "the handling of text data for a classification task. It processes the text using a tokenizer, \n",
    "converts labels to numeric values, and allows easy access to batches of data through methods like \n",
    "get_batch_labels and get_batch_texts. It can be used with PyTorch's data loaders to efficiently \n",
    "load and preprocess data for training deep learning models.\n",
    "'''\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "labels = {'business'     :0,\n",
    "          'entertainment':1,\n",
    "          'sport'        :2,\n",
    "          'tech'         :3,\n",
    "          'politics'     :4\n",
    "          }\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    #___________________________________________________________________________________________________________________\n",
    "    #\n",
    "    '''\n",
    "    The class constructor initializes the dataset. It takes a DataFrame (df) as input, which contains the data \n",
    "    to be used for training or evaluation. This data might include text and corresponding labels. Within the \n",
    "    constructor:\n",
    "    - self.labels: stores a list of labels. These labels are extracted from the 'category' column of the DataFrame df. \n",
    "                   The assumption here is that labels is a dictionary mapping label names to numeric values, and this \n",
    "                   mapping is used to convert label names into numeric labels.\n",
    "    - self.texts:  stores a list of processed text samples. Each text sample is processed using a tokenizer \n",
    "                   (in this example BertTokenizer). The text is tokenized and converted into a PyTorch tensor with \n",
    "                   specified properties like padding, truncation, and maximum length. \n",
    "    '''\n",
    "    def __init__(self, df):\n",
    "        self.labels = [labels[label] for label in df['category']]\n",
    "        self.texts  = [tokenizer(text, \n",
    "                                 padding='max_length', \n",
    "                                 max_length = 512, \n",
    "                                 truncation=True,\n",
    "                                 return_tensors=\"pt\") for text in df['text']]\n",
    "    #___________________________________________________________________________________________________________________\n",
    "    #\n",
    "    '''\n",
    "    This method returns the list of labels stored in the self.labels attribute. \n",
    "    It's essentially a getter method for the labels.\n",
    "    '''\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    #___________________________________________________________________________________________________________________\n",
    "    #\n",
    "    '''\n",
    "    This method returns the length of the dataset, which is determined by the number of labels. \n",
    "    It returns the length of the self.labels list\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    #___________________________________________________________________________________________________________________\n",
    "    #\n",
    "    '''\n",
    "    This method takes an index idx as input and returns the labels for a specific batch of data. \n",
    "    It fetches the labels for the data at the specified index idx.\n",
    "    '''\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "    #___________________________________________________________________________________________________________________\n",
    "    #\n",
    "    '''\n",
    "    This method, similar to get_batch_labels, takes an index idx and returns the preprocessed text samples \n",
    "    for a specific batch of data.\n",
    "    '''\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "    #___________________________________________________________________________________________________________________\n",
    "    #\n",
    "    '''\n",
    "    This method is used for data retrieval. It takes an index idx as input, retrieves the preprocessed \n",
    "    text samples and labels for a specific batch of data based on the index, and returns them as a tuple.\n",
    "    '''\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9671999f",
   "metadata": {},
   "source": [
    "In the above implementation, we define a variable called `labels` , which is a dictionary that maps the category in the dataframe into the id representation of our label. Notice that we also call `BertTokenizer` in the `__init__` function above to transform our input texts into the format that BERT expects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6afc0",
   "metadata": {},
   "source": [
    "After defining dataset class, let’s split our dataframe into training, validation, and test set with the proportion of 80:10:10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6dcf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd293c",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441bebac",
   "metadata": {},
   "source": [
    "When we instantiate a model with `from_pretrained()`, the model configuration and pre-trained weights of the specified model are used to initialize the model. The library also includes a number of task-specific final layers or ‘heads’ whose weights are instantiated randomly when not present in the specified pre-trained model. For example, instantiating a model with `BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)` will create a BERT model instance with encoder weights copied from the bert-base-uncased model and a randomly initialized sequence classification head on top of the encoder with an output size of 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd0ed2",
   "metadata": {},
   "source": [
    "> **NOTE**. In PyTorch, you always need to define a `forward` method for your neural network model. But you never have to call `model.forward(x)`. The `super(BertClassifier, self).__init__()` refers to the fact that this is a subclass of `nn.Module` and is inheriting all methods. In the super class, `nn.Module`, there is a `__call__` method which obtains the forward function from the subclass and calls it. For further details see the original documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert    = BertModel.from_pretrained('bert-base-cased')\n",
    "        #\n",
    "        # We add three layers on top of BERT model\n",
    "        #\n",
    "        # During training, the Dropout layer, randomly zeroes some of the elements of the input tensor with \n",
    "        # probability p using samples from a Bernoulli distribution. This has proven to be an effective technique \n",
    "        # for regularization and preventing the co-adaptation of neurons as described in the paper \n",
    "        # \"Improving neural networks by preventing co-adaptation of feature detectors\".\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # This is a simple linear layer which takes as input a BERT vector (768 dim) and as output a 5-dim real vector\n",
    "        # Remember that we have 5 categories to classify our news\n",
    "        self.linear  = nn.Linear(768, 5)\n",
    "        # Finally a relu layer is added\n",
    "        self.relu    = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        #\n",
    "        # We first feed both sequence tokens and attention masks to the bert layer and get the contextualized embeddings \n",
    "        # (hidden representations) of each token. \n",
    "        # return_dict=False: This argument specifies that the BERT model should not return the outputs in the form of a \n",
    "        # dictionary. Instead, it returns two values, the first of which is discarded (indicated by _), and the second value, \n",
    "        # pooled_output, is kept. In particular contains the embedding vector of [CLS] token. For a text classification task, \n",
    "        # it is enough to use this embedding as an input for our classifier.\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        # We then pass the pooled_output variable into a linear layer with ReLU activation function. At the end of the linear \n",
    "        # layer, we have a vector of size 5, each corresponds to a category of our labels (sport, business, politics, \n",
    "        # entertainment, and tech).\n",
    "        dropout_output   = self.dropout(pooled_output)\n",
    "        linear_output    = self.linear(dropout_output)\n",
    "        final_layer      = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f425ac35",
   "metadata": {},
   "source": [
    "The `Dataset` retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting. `DataLoader` is an iterable that abstracts this complexity for us in an easy API. When we have loaded that dataset into the `DataLoader`, we can iterate through the dataset as needed. Each iteration returns a batch of `train_features` and `train_labels` (containing `batch_size` features and labels respectively). Because we specified `shuffle=True`, after we iterate over all batches the data is shuffled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410ab06",
   "metadata": {},
   "source": [
    "> **Note: Why do we need to call zero_grad in PyTorch**. In PyTorch, for every mini-batch during the training phase, we typically want to explicitly set the gradients to zero before starting to do backpropagation (i.e., updating the Weights and biases) because PyTorch accumulates the gradients on subsequent backward passes. This accumulating behavior is convenient while training RNNs or when we want to compute the gradient of the loss summed over multiple mini-batches. So, the default action has been set to accumulate (i.e. sum) the gradients on every loss.backward() call. Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly. *Otherwise, the gradient would be a combination of the old gradient, which you have already used to update your model parameters and the newly-computed gradient. It would therefore point in some other direction than the intended direction towards the minimum (or maximum, in case of maximization objectives)*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d34942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    \n",
    "    # Next line creates two datasets, train and val, from the training and validation data, respectively. \n",
    "    # That these datasets are prepared using the custom Dataset class previously defined. \n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    \n",
    "    # Here, data loaders are created for both the training and validation datasets using PyTorch's DataLoader class. \n",
    "    # These data loaders are used to efficiently load and process data in mini-batches during training. \n",
    "    # Each mini-batch has a batch size of 2, and the training data is shuffled.\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader   = torch.utils.data.DataLoader(val,   batch_size=2)\n",
    "    \n",
    "    # We check for a GPU is available, if not we proceed with a CPU\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    # The torch.device enables you to specify the device type responsible to load a tensor into memory. \n",
    "    # The function expects a string argument specifying the device type.\n",
    "    device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # Here, the loss function (criterion) is defined as cross-entropy loss, which is often used for classification tasks. \n",
    "    # Additionally, the optimizer (Adam) is initialized with the model's parameters and the specified learning rate. \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "            model     = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "            # initiates a loop over the specified number of training epochs.\n",
    "            total_acc_train  = 0\n",
    "            total_loss_train = 0\n",
    "            #\n",
    "            # TRAINING Phase\n",
    "            #\n",
    "            # This inner loop iterates through mini-batches of training data using the train_dataloader. \n",
    "            # tqdm is a library used for creating progress bars.\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "                # For each mini-batch, the labels, attention masks (mask), and input IDs (input_id) are extracted from the train_input. \n",
    "                # These tensors are also moved to the specified device (CPU in this case).\n",
    "                train_label = train_label.to(device)\n",
    "                mask        = train_input['attention_mask'].to(device)\n",
    "                input_id    = train_input['input_ids'].squeeze(1).to(device)\n",
    "                # The model is called with the input data (input_id and mask) to obtain predictions (output) for the current \n",
    "                # mini-batch.\n",
    "                output = model(input_id, mask)\n",
    "                # The batch loss is calculated using the cross-entropy loss (criterion) and accumulated to the total_loss_train.\n",
    "                batch_loss        = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                # Accuracy is calculated by comparing the model's predictions to the ground truth labels, and the correct predictions \n",
    "                # are accumulated to total_acc_train.\n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "                # Zero out the gradients (see the above note), backpropagate the batch loss, and update the \n",
    "                # model's parameters using the optimizer.\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            #\n",
    "            # VALIDATION phase\n",
    "            #\n",
    "            # Initialize variables to accumulate validation accuracy and loss.\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            # Temporarily disable gradient tracking for the validation phase\n",
    "            with torch.no_grad():\n",
    "                # Loop through mini-batches of validation data.\n",
    "                for val_input, val_label in val_dataloader:\n",
    "                    # Extract and move validation data to the specified device.\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask      = val_input['attention_mask'].to(device)\n",
    "                    input_id  = val_input['input_ids'].squeeze(1).to(device)\n",
    "                    # The code then passes the preprocessed input data and mask to a neural \n",
    "                    # network model (model) to obtain predictions.\n",
    "                    output = model(input_id, mask)\n",
    "                    # The code calculates the loss for the mini-batch. It uses a defined loss function (criterion, \n",
    "                    # in this case CrossEntropyLoss) to compare the model's predictions (output) with the actual \n",
    "                    # validation labels (val_label). This loss is computed as batch_loss. \n",
    "                    batch_loss      = criterion(output, val_label.long())\n",
    "                    # The code accumulates the batch loss (batch_loss) into a running total, total_loss_val.\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    # It also calculates the accuracy for the mini-batch. The accuracy is determined by comparing \n",
    "                    # the model's predicted class (argmax of the output) to the actual validation labels, summing \n",
    "                    # up the correct predictions, and adding it to the running total, total_acc_val. After this \n",
    "                    # loop completes (after all mini-batches have been processed), total_loss_val will contain \n",
    "                    # the total validation loss, and total_acc_val will contain the total number of correct \n",
    "                    # predictions for the entire validation dataset. These metrics can be used to evaluate the \n",
    "                    # performance of the model on the validation data.\n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "                  \n",
    "EPOCHS = 5\n",
    "model = BertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5024e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d09e54",
   "metadata": {},
   "source": [
    "### Evaluate Model on Test Data\n",
    "\n",
    "Now that we have trained the model, we can use the test data to evaluate the model’s performance on unseen data. Below is the function to evaluate the performance of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4172d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "              test_label = test_label.to(device)\n",
    "              mask       = test_input['attention_mask'].to(device)\n",
    "              input_id   = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output     = model(input_id, mask)\n",
    "\n",
    "              acc        = (output.argmax(dim=1) == test_label).sum().item()\n",
    "              total_acc_test += acc\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    \n",
    "evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab72e5",
   "metadata": {},
   "source": [
    "## References and Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6465140f",
   "metadata": {},
   "source": [
    "- Rayyan Shaikh, **[Mastering BERT: A Comprehensive Guide from Beginner to Advanced in Natural Language Processing](https://medium.com/@shaikhrayyan123/a-comprehensive-guide-to-understanding-bert-from-beginners-to-advanced-2379699e2b51)**, Medium Blog\n",
    "---\n",
    "- Ruben Winastwan, **[Text Classification with BERT in PyTorch](https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f)**, Medium Blog\n",
    "---\n",
    "Hugging Face, **[Documentation on Transformers](https://huggingface.co/transformers/v3.3.1/training.html)**\n",
    "\n",
    "Hugging Face, **[Documentation on BERT Model](https://huggingface.co/docs/transformers/model_doc/bert)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cc9a67",
   "metadata": {},
   "source": [
    "- [What is purpose of the [CLS] token and why is its encoding output important?](https://datascience.stackexchange.com/questions/66207/what-is-purpose-of-the-cls-token-and-why-is-its-encoding-output-important)\n",
    "\n",
    "- [Paper Dissected: “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding” Explained](https://datasciencetoday.net/index.php/en-us/nlp/211-paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained)\n",
    "\n",
    "- [A Visual Guide to Using BERT for the First Time](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/) by Jay Alammar\n",
    "\n",
    "- [Painless Fine-Tuning of BERT in Pytorch](https://medium.com/swlh/painless-fine-tuning-of-bert-in-pytorch-b91c14912caa) by Kabir Ahuja\n",
    "\n",
    "- [BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/) by Chris McCormick and Nick Ryan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": "8",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
