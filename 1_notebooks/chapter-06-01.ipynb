{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1df27f",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/polyhedron-gdl/halloween-seminar-2023/tree/main/1_notebooks/chapter-06-01.ipynb\">\n",
    "        <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642c3e7",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecc891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy  as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70791adb",
   "metadata": {},
   "source": [
    "## What are Word Vectorization and Why It is so Important\n",
    "\n",
    "Word vectorization generically refers to techniques used to convert text into numbers. There may be different numerical representations of the same text. \n",
    "\n",
    "Many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing *strings* or *plain text* in their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression, etc. in broad terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39baeec2",
   "metadata": {},
   "source": [
    "## Bag-of-Words (BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1af1f",
   "metadata": {},
   "source": [
    "As we have already said, in order to perform machine learning on text, we need to transform our documents into vector representations such that we can apply numeric machine learning. This process is called feature extraction or more simply, vectorization.\n",
    "\n",
    "We will explore several choices, each of which extends or modifies the base bag-of-words model to describe semantic space. We will look at four types of vector encoding - frequency, one-hot, TF-IDF, and distributed representations - and discuss their implementations in Scikit-Learn, Gensim, and NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49233a4",
   "metadata": {},
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy  as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623ce0d",
   "metadata": {},
   "source": [
    "### Sample corpus of text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c9e83",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img align=\"left\" width=\"100\" height=\"100\" src=\"./img/text_analytics_with_python.jpg\"/> </td>\n",
    "<td> The following examples are taken from \"Text Analytics with Python\" by Dipanjan Sarkar (Apress, 2019).     </td>\n",
    "</tr></table>\n",
    "\n",
    "[Here](https://github.com/Apress/text-analytics-w-python-2e) you can find the original notebook used in the aforementioned textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'    \n",
    "]\n",
    "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, \n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7f982",
   "metadata": {},
   "source": [
    "### Simple text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd63a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0614c70",
   "metadata": {},
   "source": [
    "**Regular Expression Flags**\n",
    "\n",
    "**re.I - \n",
    "re.IGNORECASE**\n",
    "\n",
    "Perform case-insensitive matching; expressions like [A-Z] will also match lowercase letters. Full Unicode matching (such as Ü matching ü) also works unless the re.ASCII flag is used to disable non-ASCII matches. The current locale does not change the effect of this flag unless the re.LOCALE flag is also used. Corresponds to the inline flag (?i).\n",
    "\n",
    "**re.A - \n",
    "re.ASCII**\n",
    "\n",
    "Make \\\\w, \\\\W, \\\\b, \\\\B, \\\\d, \\\\D, \\\\s and \\\\S perform ASCII-only matching instead of full Unicode matching. This is only meaningful for Unicode patterns, and is ignored for byte patterns. Corresponds to the inline flag (?a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb0a99",
   "metadata": {},
   "source": [
    "![image.png](./img/3_1_text_vectorization_pic_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890dd823",
   "metadata": {},
   "source": [
    "### Frequency Vectors\n",
    "\n",
    "The simplest vector encoding model is to simply fill in the vector with the frequency of each word as it appears in the document;\n",
    "In this encoding scheme each document is represented as the multiset of the tokens that compose it and the value for each word position in the vectr is its count;\n",
    "This representation can either be a straight count encoding or a normalized encoding where each word is weighted by the total number of words in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092ad55",
   "metadata": {},
   "source": [
    "In **Scikit-Learn** The CountVectorizer transformer from the sklearn.feature_extraction model has its own internal tokenization and normalization methods. The fit method of the vectorizer expects an iterable or list of strings or file objects, and creates a dictionary of the vocabulary on the corpus. When transform is called, each individual document is transformed into a sparse array whose index tuple is the row (the document ID) and the token ID from the dictionary, and whose value is the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e38c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# get bag of words features in sparse format\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baccd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d2ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique words in the corpus\n",
    "vocab = cv.get_feature_names_out()\n",
    "# show document feature vectors\n",
    "pd.DataFrame(cv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad73797",
   "metadata": {},
   "source": [
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "Because they disregard grammar and the relative position of words in documents, frequency-based encoding methods suffer from the long tail, or Zipfian distribution, that characterizes natural language. As a result, tokens that occur very frequently are orders of magnitude more “significant” than other, less frequent ones. This can have a significant impact on some models (e.g., generalized linear models) that expect normally distributed features. \n",
    "\n",
    "A solution to this problem is **one-hot encoding**, a boolean vector encoding method that marks a particular vector index with a value of true (1) if the token exists in the document and false (0) if it does not. In other words, each element of a one-hot encoded vector reflects either the presence or absence of the token in the described text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b67cd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**TF - Term Frequency**: Just counting the number of words in each document has 1 issue: it will give more weightage to longer documents than shorter documents. To avoid this, we can use frequency i.e. #count(word) / #Total words, in each document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d3cb15",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency\n",
    "\n",
    "The bag-of-words representations that we have explored so far only describe a document in a standalone fashion, not taking into account the context of the corpus. \n",
    "A better approach would be to consider the relative frequency or rareness of tokens in the document against their frequency in other documents. The central insight is that meaning is most likely encoded in the more rare terms from a document.\n",
    "\n",
    "TF-IDF, term frequency-inverse document frequency, encoding normalizes the frequency of tokens in a document with respect to the rest of the corpus. \n",
    "This encoding approach accentuates terms that are very relevant to a specific instance, as shown in Figure, where the token studio has a higher relevance to this document since it only appears there.\n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. \n",
    "It is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient: \n",
    "\n",
    "\\begin{equation}idf(t,D) = \\log \\frac{N}{\\vert \\{  d \\in D: t \\in d\\}\\vert} \\end{equation}  \n",
    "\n",
    "where the numerator ($N$) is the total number of documents in the corpus and the denominator is the number of documents where the term $t$ appears.\n",
    "\n",
    "Then tf-idf is calculated as: \n",
    "\n",
    "\\begin{equation} tfidf(t,d,D) = tf(t,d) \\cdot idf(t,D)\\end{equation}\n",
    "\n",
    "A high weight in tf-idf is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms.  Since the ratio inside the idf's log function is always greater than or equal to 1, the value of idf (and tf-idf) is greater than or equal to 0. As a term appears in more documents, the ratio inside the logarithm approaches 1, bringing the idf and tf-idf closer to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a3b0b",
   "metadata": {},
   "source": [
    "![image.png](./img/tf-idf-0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344bcc72",
   "metadata": {},
   "source": [
    "A simple example might serve to explain the structure of the TDM more clearly. Assume we have a simple corpus consisting of two documents, Doc1 and Doc2, with the following content:\n",
    "\n",
    "Doc1 = \"I like databases\"\n",
    "Doc2 = \"I dislike databases\",\n",
    "\n",
    "then the document-term matrix would be:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f668d879",
   "metadata": {},
   "source": [
    "![image.png](./img/tf-idf-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccfa1ff",
   "metadata": {},
   "source": [
    "Clearly there is nothing special about rows and columns – we could just as easily transpose them. If we did so, we’d get a term document matrix (TDM) in which the terms are rows and documents columns. One can work with either a DTM or TDM. Using the raw count of a term in a document, i.e. the number of times that term t occurs in document d, is the simplest choice to measure the term frequency $tf(t,d)$. If we denote the raw count by $f_{t,d}$, then the simplest $tf$ scheme is $tf(t,d) = f_{t,d}$. Other possibilities include\n",
    "\n",
    "- Boolean \"frequencies\": $tf(t,d) = 1$ if $t$ occurs in $d$ and $0$ otherwise;\n",
    "- Term frequency adjusted for document length : $f_{t,d} \\big/ \\text{(number of words in d)}$;\n",
    "- Logarithmically scaled frequency: $tf(t,d) = \\log (1 + f_{t,d})$;\n",
    "- Augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the raw frequency of the most occurring term in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1cfa05",
   "metadata": {},
   "source": [
    "![image.png](./img/3_1_text_vectorization_pic_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef75c7",
   "metadata": {},
   "source": [
    "Scikit-Learn provides a transformer called the TfidfVectorizer in the module called **feature_extraction.text** for vectorizing documents with TF–IDF scores. Under the hood, the TfidfVectorizer uses the CountVectorizer estimator we used to produce the bag-of-words encoding to count occurrences of tokens, followed by a TfidfTransformer, which normalizes these occurrence counts by the inverse document frequency. \n",
    "\n",
    "The input for a TfidfVectorizer is expected to be a sequence of filenames, file-like objects, or strings that contain a collection of raw documents, similar to that of the CountVectorizer. As a result, a default tokenization and preprocessing method is applied unless other functions are specified. The vectorizer returns a sparse matrix representation in the form of ((doc, term), tfidf) where each key is a document and term pair and the value is the TF–IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a5b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., norm='l2',\n",
    "                     use_idf=True, smooth_idf=True)\n",
    "tv_matrix = tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names()\n",
    "pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73276e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = 'the sky is green today'\n",
    "\n",
    "pd.DataFrame(np.round(tv.transform([new_doc]).toarray(), 2), \n",
    "             columns=tv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58868fea",
   "metadata": {},
   "source": [
    "## Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4e2a5",
   "metadata": {},
   "source": [
    "When you have vectorized your text, we can try to define a distance metric such that documents that are closer together in feature space are more similar. There are a number of different measures that can be used to determine document similarity; \n",
    "Several are illustrated in Figure. Fundamentally, each relies on our ability to imagine documents as points in space, where the relative closeness of any two documents is a measure of their similarity.\n",
    "\n",
    "We can measure vector similarity with cosine distance, using the cosine of the angle between the two vectors to assess the degree to which they share the same orientation. In effect, the more parallel any two vectors are, the more similar the documents will be (regardless of their magnitude)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19fad09",
   "metadata": {},
   "source": [
    "<!--\n",
    "<div>\n",
    "<img src=\"./img/3_1_text_vectorization_pic_9.png\" width=\"500\"/>\n",
    "</div>\n",
    "-->\n",
    "![pic](./img/3_1_text_vectorization_pic_9.png)\n",
    "*(image source: Bengfort B. et al. \"Text Analysis with Python\")*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0696c72",
   "metadata": {},
   "source": [
    "Mathematically, Cosine similarity metric measures the cosine of the angle between two n-dimensional vectors projected in a multi-dimensional space. The Cosine similarity of two documents will range from 0 to 1. If the Cosine similarity score is 1, it means two vectors have the same orientation. The value closer to 0 indicates that the two documents have less similarity.\n",
    "\n",
    "The mathematical equation of Cosine similarity between two non-zero vectors is: \n",
    "\n",
    "\\begin{equation} \\text{similarity} = \\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\left\\Vert\\mathbf{A}\\right\\Vert \\,\\left\\Vert\\mathbf{B}\\right\\Vert} = \\frac{\\sum\\limits_{i=1}^n A_iB_i}{\\sum\\limits_{i=1}^n A_i^2 \\sum\\limits_{i=1}^n B_i^2} \\end{equation}\n",
    "\n",
    "Let’s see an example of how to calculate the cosine similarity between two text document. The common way to compute the Cosine similarity is to first we need to count the word occurrence in each document. To count the word occurrence in each document, we can use **CountVectorizer** or **TfidfVectorizer** functions that are provided by Scikit-Learn library.\n",
    "\n",
    "doc_1 = \"Data is the oil of the digital economy\" \n",
    "doc_2 = \"Data is a new oil\" \n",
    "\n",
    "and consider the following frequency matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45755e63",
   "metadata": {},
   "source": [
    "<!--\n",
    "<img src='./img/tf-idf-2.png'>\n",
    "-->\n",
    "![image.png](./img/tf-idf-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b514f5",
   "metadata": {},
   "source": [
    "We can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982fc4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = \"Data is the oil of the digital economy\" \n",
    "doc_2 = \"Data is a new oil\" \n",
    "data = [doc_1, doc_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "vector_matrix = count_vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d0818",
   "metadata": {},
   "source": [
    "Here, is the unique tokens list found in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d99a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = count_vectorizer.get_feature_names_out()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c0b206",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08bbe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(matrix, tokens):\n",
    "\n",
    "    doc_names = [f'doc_{i+1}' for i, _ in enumerate(matrix)]\n",
    "    df = pd.DataFrame(data=matrix, index=doc_names, columns=tokens)\n",
    "    return(df)\n",
    "\n",
    "create_dataframe(vector_matrix.toarray(),tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf97f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
    "create_dataframe(cosine_similarity_matrix,['doc_1','doc_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd9ca9",
   "metadata": {},
   "source": [
    "Let’s check the cosine similarity with `TfidfVectorizer`, and see how it change over `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05c0d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer()\n",
    "vector_matrix = Tfidf_vect.fit_transform(data)\n",
    "\n",
    "tokens = Tfidf_vect.get_feature_names_out()\n",
    "create_dataframe(vector_matrix.toarray(),tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
    "create_dataframe(cosine_similarity_matrix,['doc_1','doc_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967774d5",
   "metadata": {},
   "source": [
    "Here, using `TfidfVectorizer` we get the cosine similarity between doc_1 and doc_2 is 0.32.  Where the `CountVectorizer` has returned the cosine similarity of doc_1 and doc_2 is 0.47. This is because `TfidfVectorizer` penalized the most frequent words in the document such as stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859ced2",
   "metadata": {},
   "source": [
    "Now, the **distance** can be defined as \n",
    "\n",
    "\\begin{equation}\n",
    "d =1-\\mathrm{CosineSimilarity} \n",
    "\\end{equation}\n",
    "\n",
    "The intuition behind this is that if 2 vectors are perfectly the same then similarity is 1 (angle=0) and thus, distance is 0 (1-1=0).\n",
    "\n",
    "\n",
    "Let's apply the same analysis to our toy corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd759d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(vector_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4f1104",
   "metadata": {},
   "source": [
    "**Clustering documents using similarity features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "Z = linkage(similarity_matrix, 'ward')\n",
    "pd.DataFrame(Z, columns=['Document\\Cluster 1', 'Document\\Cluster 2', \n",
    "                         'Distance', 'Cluster Size'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e8164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Data point')\n",
    "plt.ylabel('Distance')\n",
    "dendrogram(Z)\n",
    "plt.axhline(y=1.0, c='k', ls='--', lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "max_dist = 1.0\n",
    "\n",
    "cluster_labels = fcluster(Z, max_dist, criterion='distance')\n",
    "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3ee4a",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc6bb2c",
   "metadata": {},
   "source": [
    "Humans have always excelled at understanding languages. It is easy for humans to understand the relationship between words but for computers, this task may not be simple. Word embeddings are basically a form of word representation that bridges the human understanding of language to that of a machine. For example, we humans understand the words like king and queen, man and woman, tiger and tigress have a certain type of relation between them but how can a computer figure this out?\n",
    "\n",
    "The different encoding we have discussed so far is arbitrary as it does not capture any relationship between words. \n",
    "It can be challenging for a model to interpret, for example, a linear classifier learns a single weight for each feature. \n",
    "Because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful.\n",
    "\n",
    "It should be nice to have representations of text in an n-dimensional space where words that have the same meaning have a similar representation. Meaning that two similar words are represented by almost similar vectors that are very closely placed in a vector space. \n",
    "\n",
    "Thus when using word embeddings, all individual words are represented as real-valued vectors in a predefined vector space. \n",
    "Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network. Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google.\n",
    "\n",
    "The concept of embeddings arises from a branch of Natural Language Processing called - “Distributional Semantics”. It is based on the simple intuition that: ***Words that occur in similar contexts tend to have similar meanings***. In other words, a word’s meaning is given by the words that it appears frequently with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b9d50b",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92315c13",
   "metadata": {},
   "source": [
    "Word2vec is a method to efficiently create word embeddings by using a two-layer neural network.  \n",
    "It was developed by Tomas Mikolov, et al. at Google in 2013 as a response to make the neural-network-based training of the embedding more efficient and since then has become the de facto standard for developing pre-trained word embedding. \t\t  \n",
    "The input of word2vec is a text corpus and its output is a set of vectors known as feature vectors that represent words in that corpus. \n",
    "The Word2Vec objective function causes the words that have a similar context to have similar embeddings. \n",
    "Thus in this vector space, these words are really close. Mathematically, the cosine of the angle (Q) between such vectors should be close to 1, i.e. angle close to 0.\n",
    "\n",
    "Word2vec is not a single algorithm but a combination of two techniques - **CBOW(Continuous bag of words)** and **Skip-gram** model. Both these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both these techniques learn weights which act as word vector representations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d899ea0",
   "metadata": {},
   "source": [
    "**CBOW**. \n",
    "\n",
    "CBOW predicts the probability of a word to occur **given the words surrounding it**. We can consider a single word or a group of words. But for simplicity, we will take a single context word and try to predict a single target word.\n",
    "The English language contains almost 1.2 million words, making it impossible to include so many words in our example. So we will consider a small example in which we have only four words i.e. ***live***, ***home***, ***they*** and ***at***. For simplicity, we will consider that the corpus contains only one sentence, that being, ***They live at home***. First, we convert each word into a one-hot encoding form. Also, we'll not consider all the words in the sentence but ll only take certain words that are in a window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f6605",
   "metadata": {},
   "source": [
    "![image.png](./img/word-embedding-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d9dea",
   "metadata": {},
   "source": [
    "For example for a window size equal to three, we only consider three words\n",
    "in a sentence. The middle word is to be predicted and the surrounding two\n",
    "words are fed into the neural network as context. The window is then slid\n",
    "and the process is repeated again. Finally, after training the network repeatedly by sliding the window a\n",
    "shown above, we get weights which we use to get the embeddings as\n",
    "shown below. Usually, we take a window size of around 8-10 words and have a\n",
    "vector size of 300."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b585ff",
   "metadata": {},
   "source": [
    "![image.png](./img/word-embedding-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de47142",
   "metadata": {},
   "source": [
    "#### Implementing a word2vec model using a CBOW (Continuous Bag of Words) neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba03c128",
   "metadata": {},
   "source": [
    "This code process text data from Lewis Carroll's \"*Alice's Adventures in Wonderland*\", which is part of the Gutenberg corpus available through the Natural Language Toolkit (NLTK) in Python. The code performs several text preprocessing steps to clean and normalize the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96c9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code imports the gutenberg corpus from the NLTK and the punctuation constant from the string module.\n",
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "\n",
    "# loads the text from the \"carroll-alice.txt\" file in the Gutenberg corpus and stores it in the book variable. \n",
    "# The gutenberg.sents method is used to retrieve the sentences from the book.\n",
    "book         = gutenberg.sents('carroll-alice.txt') \n",
    "#  creates a remove_terms variable, which is a string containing all punctuation marks and the digits 0-9. \n",
    "# This variable is intended to be used to remove these characters from the text.\n",
    "remove_terms = punctuation + '0123456789'\n",
    "\n",
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb7ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goes through each sentence in the book and performs the following operations:\n",
    "# - It converts each word in the sentence to lowercase.\n",
    "# - It removes any word that is in the remove_terms string (punctuation and digits).\n",
    "# The result is a list of lists, where each inner list represents a sentence, and each sentence consists \n",
    "# of words in lowercase without punctuation and digits.\n",
    "norm_doc = [[word.lower() for word in sent if word not in remove_terms] for sent in book]\n",
    "# Joins the words in each sentence into a single string, using a space as a separator. \n",
    "# This creates a list of normalized, space-separated sentences.\n",
    "norm_doc = [' '.join(tok_sent) for tok_sent in norm_doc]\n",
    "norm_doc = filter(None, normalize_corpus(norm_doc))\n",
    "# Filters out sentences that have fewer than 3 words (as determined by splitting the sentence into words using spaces).\n",
    "norm_doc = [tok_sent for tok_sent in norm_doc if len(tok_sent.split()) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc145ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total lines:', len(book))\n",
    "print('\\nSample line:', book[100])\n",
    "print('\\nProcessed line:', norm_doc[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e090f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'to_categorical' for one-hot encoding and 'text' and 'sequence' modules for text preprocessing.\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence\n",
    "# create a tokenizer object by calling text.Tokenizer(). A tokenizer is used to convert text into a \n",
    "# sequence of numbers (word indices) and perform various text processing operations.\n",
    "tokenizer = text.Tokenizer()\n",
    "# fits the tokenizer on the text data stored in the norm_doc variable. The fit_on_texts method of \n",
    "# the tokenizer is used for this purpose. Fitting the tokenizer means that it learns the vocabulary \n",
    "# of the text and assigns a unique integer index to each word based on its frequency in the given text.\n",
    "tokenizer.fit_on_texts(norm_doc)\n",
    "# After fitting the tokenizer, the code extracts the word-to-index mapping using the word_index attribute \n",
    "# of the tokenizer. This mapping is stored in the word2id variable.\n",
    "word2id = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65cf5e7",
   "metadata": {},
   "source": [
    "The result of this code is that you have a `word2id` dictionary where words from the `norm_doc` text data are mapped to unique integer indices. These integer indices can be used for various purposes, such as training machine learning models, including neural networks, that require numerical input. In the context of natural language processing (NLP), this is an essential step as it allows you to represent text data in a format suitable for model training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id.get('alice')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d697d0d",
   "metadata": {},
   "source": [
    "see [here](https://medium.com/@bramblexu/why-tokenizer-in-keras-reserve-word-index-0-33b2c634cca2) for an explanation of padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86639ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id['PAD'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a7946",
   "metadata": {},
   "source": [
    "To find a word from an index using the word2id dictionary, you can reverse the dictionary by creating a new dictionary where the keys are the integer indices and the values are the corresponding words. Here's how you can do it in Python using [dictionary comprehension](https://www.datacamp.com/tutorial/python-dictionary-comprehension):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverses the word2id dictionary by using a dictionary comprehension. \n",
    "# This line of code swaps the keys and values of word2id, so the resulting id2word dictionary \n",
    "# maps integer indices (IDs) to words. It allows you to look up words using their numerical indices.\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "# This line of code creates a list of lists, where each inner list represents a document in the \n",
    "# norm_doc data, and the elements of these inner lists are word indices. It uses list comprehensions \n",
    "# and the text.text_to_word_sequence method from Keras to tokenize the text documents and map each \n",
    "# word to its corresponding ID using the word2id dictionary.\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_doc]\n",
    "# This is the total number of unique words in the vocabulary, which is determined by the length of \n",
    "# the word2id dictionary.\n",
    "vocab_size  = len(word2id)\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024faffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dca8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in wids[0]:\n",
    "    print(id2word[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a3c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimension of the word embeddings. In this case, it is set to 100, meaning each word will \n",
    "# be represented by a vector of length 100.\n",
    "embed_size  = 100\n",
    "# Define the window size for a word embedding algorithm (e.g., Word2Vec). It defines the context window \n",
    "# for word prediction.\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9279eb80",
   "metadata": {},
   "source": [
    "The function `generate_context_word_pairs` is a [generator](https://realpython.com/introduction-to-python-generators/) function that takes a text corpus and other parameters related to word embedding using the Word2Vec algorithm. It generates context-word pairs for training a Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    '''\n",
    "    Input parameters:\n",
    "    \n",
    "    - corpus:      This parameter represents the input text corpus, which is a list of lists, where each inner list \n",
    "                   represents a sentence or document, and its elements are the words in that sentence.\n",
    "    - window_size: It specifies the size of the context window, which determines how many words around a target \n",
    "                   word will be considered as context words. The context window size is typically set to a fixed value, \n",
    "                   like 2, to capture nearby context.\n",
    "    - vocab_size:  The vocabulary size represents the total number of unique words in the corpus. This is used to \n",
    "                   one-hot encode the label words.\n",
    "    '''\n",
    "    # context_length is calculated as window_size * 2, which gives the total number of words in the context \n",
    "    # of a target word (before and after the target word).\n",
    "    context_length = window_size*2\n",
    "    # This loop iterates over each sentence (list of words) in the corpus\n",
    "    for words in corpus:\n",
    "        # the number of words in the current sentence\n",
    "        sentence_length = len(words)\n",
    "        # This loop iterates through each word in the sentence along with its index \n",
    "        for index, word in enumerate(words):\n",
    "            # we initialize two empty lists: context_words and label_word. context_words will hold the words \n",
    "            # in the context of the target word, and label_word will hold the target word itself.\n",
    "            context_words = []\n",
    "            label_word    = []  \n",
    "            # The context window includes words before and after the target word.\n",
    "            start         = index - window_size\n",
    "            end           = index + window_size + 1\n",
    "            # appends the words in the context window (excluding the target word itself) to the context_words \n",
    "            # list using a list comprehension.\n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            # The target word is appended to the label_word list.\n",
    "            label_word.append(word)\n",
    "            # the following line converts context_words into a fixed-size sequence of word indices using \n",
    "            # sequence.pad_sequences. This is done because different sentences may have different numbers \n",
    "            # of context words, and it's necessary to ensure that they all have the same length. Padding \n",
    "            # is added as needed.\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            # The target word (label) is one-hot encoded using to_categorical with a vocabulary size of \n",
    "            # vocab_size. This means that the target word is represented as a binary vector where all \n",
    "            # elements are zero except for the index corresponding to the word, which is set to one.\n",
    "            y = to_categorical(label_word, vocab_size)\n",
    "            # Finally, the function yields the context-word pairs as tuples (x, y), where x is the \n",
    "            # context and y is the one-hot encoded label. This allows you to use this generator function \n",
    "            # in a training loop for Word2Vec or similar word embedding models. The use of a generator \n",
    "            # helps in memory efficiency when working with large corpora.\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab77440",
   "metadata": {},
   "source": [
    "The following code prints the first 10 context-word pairs with their corresponding context and target words. This is useful for inspecting and verifying the context and target pairs generated by the Word2Vec training data. It can be particularly helpful for debugging and understanding the training data before training a Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa105fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938eaa91",
   "metadata": {},
   "source": [
    "#### Build CBOW Deep Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45017e41",
   "metadata": {},
   "source": [
    "Now we can set up a CBOW model for word embeddings. The model takes context words as input, computes the mean of their embeddings, and then predicts the target word using a softmax output layer. It's a simple but effective architecture for learning word representations from a large text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9bd918",
   "metadata": {},
   "source": [
    "In Keras, `keras.backend` is a module that provides a backend-agnostic interface for low-level operations in neural networks. It allows Keras to work with multiple backend engines, such as TensorFlow, Theano, and CNTK, without needing to change the high-level code. The `keras.backend` module abstracts the operations, allowing Keras to be used with different computational backends seamlessly.\n",
    "\n",
    "Some of the functions and utilities you can find in `keras.backend` include:\n",
    "\n",
    "1. **Tensor operations**: `keras.backend` provides a consistent way to perform tensor operations, such as addition, subtraction, multiplication, and more, across different backend engines.\n",
    "\n",
    "2. **Math functions**: It includes mathematical functions like `mean`, `sum`, `abs`, and others, which work consistently across different backends.\n",
    "\n",
    "3. **Variable handling**: You can create and manipulate variables and placeholders with functions like `variable`, `placeholder`, and `update`.\n",
    "\n",
    "4. **Control flow**: `keras.backend` supports control flow operations like `switch`, `if_else`, and `not_equal`.\n",
    "\n",
    "5. **Convolution operations**: You can find functions for 2D and 3D convolutions, pooling operations, and more.\n",
    "\n",
    "6. **Loss functions**: Many commonly used loss functions for different types of tasks, such as classification, regression, and sequence-to-sequence tasks, are available in `keras.backend`.\n",
    "\n",
    "7. **Activation functions**: Functions like `sigmoid`, `tanh`, `relu`, and others can be found for defining activation functions.\n",
    "\n",
    "8. **Initialization methods**: `keras.backend` provides functions for initializing model weights, such as `zeros`, `ones`, and `glorot_normal`.\n",
    "\n",
    "The purpose of `keras.backend` is to ensure that the high-level Keras code can be written independently of the underlying backend engine. When you choose a backend (e.g., TensorFlow or Theano) for your Keras installation, Keras will use the appropriate backend-specific functions from `keras.backend` to perform the operations. This abstraction allows you to write code that is portable across different backend engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e05856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "# A Sequential model is created and stored in the cbow variable. Sequential models are a linear stack of layers, where you \n",
    "#can add layers one after the other.\n",
    "cbow = Sequential()\n",
    "# An Embedding layer is added to the model. \n",
    "# - input_dim is set to vocab_size, which specifies the size of the vocabulary. This layer is responsible for \n",
    "#   learning word embeddings, mapping each word to a dense vector of embed_size dimensions.\n",
    "# - output_dim is set to embed_size, determining the dimensionality of the word embeddings.\n",
    "# - input_length is set to window_size*2, indicating the size of the input context window around the target word.\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "# The Lambda layer is used to compute the mean of the embeddings within the context window. This step helps \n",
    "# aggregate the context information. It takes the mean along axis=1, which means it calculates the mean for \n",
    "# each word's embedding vectors. output_shape is set to (embed_size,) to specify the shape of the output.\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "# This layer is the output layer, and it has vocab_size neurons, which corresponds to the size of the vocabulary.\n",
    "# The activation function used in this layer is 'softmax', which is typical for language modeling tasks like \n",
    "# predicting the next word in a sequence.\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "# The model is compiled. We specifie the loss function as categorical cross-entropy, which is suitable for \n",
    "# multi-class classification tasks like predicting words from a vocabulary. The optimizer is set to 'rmsprop', \n",
    "# which is a popular optimization algorithm for training neural networks.\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11843136",
   "metadata": {},
   "source": [
    "Now we can train our model. The following code iterates through five epochs, processing context-word pairs in each epoch, and updating the model's weights to minimize the loss. The loss indicates how well the model is performing in predicting the target word from its context. After each epoch, it prints the epoch number and the loss, providing an indication of how the model's performance is improving over time through training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091df9f6",
   "metadata": {},
   "source": [
    "> **Note**: Keras’ `train_on_batch` function accepts a single batch of data, perform backpropagation on it and then update the model parameters. The batch of data can be any size, doesn’t require to define explicitly. When you want to train the model with your own custom rule and want to take the entire control over model training, you can use `Keras.train_on_batch()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4b6cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, nepochs):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    # This is the inner loop that iterates through context-word pairs generated by the generate_context_word_pairs function. \n",
    "    # It uses the previously defined generator to yield these pairs from the training data.\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i    += 1\n",
    "        # This line computes the loss for the current batch of context-word pairs and adds it to the running total loss. \n",
    "        # cbow is the CBOW model, and train_on_batch is used to perform a single update of the model's weights based on \n",
    "        # the batch.\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 1000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be8272e",
   "metadata": {},
   "source": [
    "The following code snippet extracts the word embeddings (weights) learned by the Continuous Bag of Words (CBOW) model and then creating a DataFrame to display these embeddings. Here's a step-by-step explanation of the code:\n",
    "\n",
    "1. `weights = cbow.get_weights()[0]`: This line retrieves the model's weights. The `cbow.get_weights()` method returns a list of all trainable parameters of the model, and in this case, we're interested in the first element of that list, which corresponds to the weights of the Embedding layer. The `weights` variable now contains the word embeddings, including both known and unknown words (e.g., padding).\n",
    "\n",
    "2. `weights = weights[1:]`: This line excludes the first row of the `weights` array. The reason for this is that the first row typically corresponds to the padding token or an out-of-vocabulary token, which is not part of the actual vocabulary. By removing this row, the `weights` array contains only the embeddings for words in the vocabulary.\n",
    "\n",
    "3. `print(weights.shape)`: This line prints the shape of the `weights` array. It provides information about the number of words in the vocabulary and the dimensionality of the word embeddings.\n",
    "\n",
    "4. `pd.DataFrame(weights, index=list(id2word.values())[1:]).head()`: This part of the code uses the Pandas library to create a DataFrame from the `weights` array, and it shows the first few rows. Here's what each part of this line does:\n",
    "   - `pd.DataFrame(weights, ...)` creates a DataFrame with the word embeddings stored in the `weights` array.\n",
    "   - `index=list(id2word.values())[1:]` sets the row labels of the DataFrame. It uses `id2word` dictionary as a map from word indices to words, and `list(id2word.values())[1:]` extracts the words from the vocabulary (excluding the padding or unknown word at index 0).\n",
    "   - `.head()` displays only the first few rows of the DataFrame.\n",
    "\n",
    "The end result of this code is a DataFrame that shows the word embeddings for the words in the vocabulary, which can be useful for visualizing and understanding how the model has represented different words in its embedding space. It's often used for tasks like word similarity and visualizing the relationships between words in the learned embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11965ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb6bef",
   "metadata": {},
   "source": [
    "Now we can compute the pairwise Euclidean distances between word embeddings to find contextually similar words in the learned embedding space. It uses the `euclidean_distances` function from scikit-learn to calculate the distances. Here's a step-by-step explanation of the code:\n",
    "\n",
    "1. `from sklearn.metrics.pairwise import euclidean_distances`: This line imports the `euclidean_distances` function from scikit-learn, a Python library for machine learning and data analysis.\n",
    "\n",
    "2. `distance_matrix = euclidean_distances(weights)`: The `euclidean_distances` function is used to compute the pairwise Euclidean distances between word embeddings stored in the `weights` array. The `weights` array is assumed to be a 2D Numpy array where each row represents the word embedding vector for a specific word. This line computes a square distance matrix where `distance_matrix[i][j]` represents the Euclidean distance between the embeddings of words with indices `i` and `j`.\n",
    "\n",
    "3. `print(distance_matrix.shape)`: This line prints the shape of the `distance_matrix` to show the dimensions of the distance matrix. It's essentially showing the number of words for which distances have been calculated.\n",
    "\n",
    "4. `similar_words = {...}`: This part of the code creates a dictionary `similar_words` that will contain contextually similar words for a given set of search terms.\n",
    "\n",
    "5. The dictionary comprehension `[search_term: ... for search_term in ['cat', 'caterpillar', 'hat', 'alice', 'queen', 'heart']]` loops through a list of search terms, and for each term, it calculates contextually similar words by performing the following steps:\n",
    "\n",
    "   - `distance_matrix[word2id[search_term]-1]`: This retrieves the row in the distance matrix corresponding to the search term. `word2id[search_term]` is assumed to be a dictionary that maps words to their indices. `-1` is used because indices typically start at 0, and `word2id` might start word indices at 1.\n",
    "\n",
    "   - `.argsort()[1:6]+1`: This line sorts the indices of the words in ascending order of their Euclidean distances to the search term. `[1:6]` takes the top 5 closest words (excluding the search term itself), and `+1` is used to convert the indices back to start from 1 (assuming they started from 1 in the `word2id` dictionary).\n",
    "\n",
    "   - `[id2word[idx] for idx in ...]`: For each of the top 5 closest word indices, this code retrieves the corresponding word from the `id2word` dictionary, which maps word indices to words.\n",
    "\n",
    "The result is a dictionary that associates each search term with a list of contextually similar words. It's a way to explore the words that are semantically close to the given search terms in the learned embedding space. This can be used for tasks like word similarity or for understanding how words are distributed in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265205f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# compute pairwise distance matrix\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "# view contextually similar words\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['cat', 'caterpillar', 'hat', 'alice', 'queen', 'heart']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d258458",
   "metadata": {},
   "source": [
    "In the last section, we perform dimensionality reduction and visualization of word embeddings using t-Distributed Stochastic Neighbor Embedding (t-SNE). It uses t-SNE to reduce the high-dimensional word embeddings into a two-dimensional space, which allows for the visualization of semantically similar words in a 2D scatter plot. Here's a breakdown of the code:\n",
    "\n",
    "1. **Import the necessary library**:\n",
    "   - `from sklearn.manifold import TSNE`: Import the t-SNE implementation from scikit-learn's `manifold` module.\n",
    "\n",
    "2. **Prepare data**:\n",
    "   - `words = sum([[k] + v for k, v in similar_words.items()], [])`: This line combines the original search terms with their contextually similar words and flattens them into a single list. `similar_words` is the dictionary created in the previous code snippet.\n",
    "   - `words_ids = [word2id[w] for w in words]`: This line maps the words to their corresponding word indices using the `word2id` dictionary.\n",
    "   - `word_vectors = np.array([weights[idx] for idx in words_ids])`: It retrieves the word vectors from the `weights` array based on the word indices and creates a Numpy array of word vectors. The `word_vectors` array now contains the word embeddings for the words of interest.\n",
    "\n",
    "3. **Print information**:\n",
    "   - `print('Total words:', len(words), '\\tWord Embedding shapes:', word_vectors.shape)`: This line prints the total number of words and the shape of the `word_vectors` array. It provides information about the dimensionality of the word vectors and the number of words being visualized.\n",
    "\n",
    "4. **t-SNE embedding**:\n",
    "   - `tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=3)`: Create a t-SNE object with specified parameters:\n",
    "     - `n_components=2`: Embed the data into a 2D space.\n",
    "     - `random_state=0`: Set the random seed for reproducibility.\n",
    "     - `n_iter=10000`: The number of iterations for the optimization process.\n",
    "     - `perplexity=3`: A parameter controlling the balance between preserving local and global structure in the data.\n",
    "\n",
    "5. **Fit t-SNE**:\n",
    "   - `T = tsne.fit_transform(word_vectors)`: Apply t-SNE to reduce the dimensionality of the word vectors and create a 2D representation stored in the `T` array.\n",
    "\n",
    "6. **Plot the scatter plot**:\n",
    "   - `plt.figure(figsize=(14, 8))`: Create a figure for the scatter plot with the specified size.\n",
    "   - `plt.scatter(T[:, 0], T[:, 1], c='steelblue', edgecolors='k')`: Create a scatter plot with `T[:, 0]` as the x-coordinates and `T[:, 1]` as the y-coordinates. The `c` parameter sets the color of the points, and `edgecolors` sets the color of the point boundaries.\n",
    "\n",
    "7. **Label the points**:\n",
    "   - `for label, x, y in zip(labels, T[:, 0], T[:, 1]):`: This loop iterates through the labels and coordinates and labels the points in the scatter plot.\n",
    "   - `plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')`: For each point, it annotates the plot with the word label, slightly offset from the point's coordinates for better visibility.\n",
    "\n",
    "The result is a scatter plot that visualizes the word embeddings in a 2D space, showing the relationships and similarities between words. This can be a valuable tool for understanding how words are distributed in the word embedding space and for exploring semantic relationships among words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "words_ids = [word2id[w] for w in words]\n",
    "word_vectors = np.array([weights[idx] for idx in words_ids])\n",
    "print('Total words:', len(words), '\\tWord Embedding shapes:', word_vectors.shape)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(word_vectors)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='steelblue', edgecolors='k')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6efd5f5",
   "metadata": {},
   "source": [
    "**Skip-gram**\n",
    "\n",
    "The Skip-gram model tries to predict the source context words (surrounding words) given a target word (the centre word). The working is conceptually similar to the CBOW, there is just a difference in the architecture of its NN and in the way the weight matrix is generated:\t "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651bac1d",
   "metadata": {},
   "source": [
    "![image.png](./img/3_1_text_vectorization_pic_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19d6848",
   "metadata": {},
   "source": [
    "# References and Credits\n",
    "\n",
    "\n",
    "\n",
    "Text Analytics with Python\n",
    "A Practitioner's Guide to Natural Language Processing\n",
    "Authors: Sarkar, Dipanjan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": "6",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
