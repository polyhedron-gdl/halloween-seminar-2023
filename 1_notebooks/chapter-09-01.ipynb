{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf155872",
   "metadata": {},
   "source": [
    "# OpenAI API Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ce19f",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54851371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d6d1b",
   "metadata": {},
   "source": [
    "To use OpenAI GPT-3 in Python, you will need to install the openai-python package. You can simply install the package with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c6ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d95219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OpenAI library\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d53f2f",
   "metadata": {},
   "source": [
    "First of all we have to set up the OpenAI API key for authentication when making API requests to OpenAI's services. In this context, an API key is a unique authentication token that provides access to OpenAI's services. By setting the API key in this manner, you ensure that your code can authenticate itself when making requests to OpenAI's API endpoints. This is a common practice in working with APIs, as it helps maintain security and access control to the API services.\n",
    "\n",
    "It's important to note that the actual API key value **should be kept secure and not hard-coded directly in the code**. Instead, it is typically stored as an **environment variable** or in a configuration file, as shown in this code, to separate sensitive information from the source code and prevent accidental exposure of the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f2bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your OpenAI API key. The value of the API key is obtained using the os.getenv('OPENAI_API_KEY') \n",
    "# function call, which is used to retrieve the value of the environment variable named 'OPENAI_API_KEY'.\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b42758",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e989b",
   "metadata": {},
   "source": [
    "**Generate creative text** \n",
    "\n",
    "This code sends a request to the OpenAI API for text generation. The response \n",
    "from the API will contain the generated text. The parameters are the following:\n",
    "\n",
    "- `model`       : Specifies the model to use for text generation. In this case, it's using the \"text-davinci-003\" model, \n",
    "                which is one of the GPT-3 models provided by OpenAI. This model is optimized for text generation tasks.     \n",
    "- `prompt`      : The prompt parameter contains the input text or query that serves as the starting point for text generation. \n",
    "                The model will use this text to generate creative responses or completions.\n",
    "- `temperature` : The `temperature` parameter controls the level of randomness in the generated text. A lower value, \n",
    "                such as 0.1, makes the text more deterministic and focused, while a higher value introduces randomness \n",
    "                and creativity into the responses.\n",
    "- `max_tokens`  : The `max_tokens` parameter sets an upper limit on the length of the generated text. In this case, it's \n",
    "                set to 1000 tokens, ensuring that the response doesn't exceed this length.\n",
    "\n",
    "After making the API request with these parameters, the response object will contain the generated text. You can access the generated text from response.choices[0].text. This text will be a creative completion of the given prompt based on the model's understanding and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "feb726e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt for text generation\n",
    "prompt = 'You are a gastronomy expert. Write a short journalistic article that enthusiastically describes Bolognese cuisine'\n",
    "#prompt = 'You are a physicist. Write a short journalistic article that describe the entanglement phenomenon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d2e6f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "    model       = \"text-davinci-003\",\n",
    "    prompt      = prompt,\n",
    "    temperature = 0.1,\n",
    "    max_tokens  = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c13d3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "Bolognese cuisine is a classic Italian favorite that has been delighting taste buds for centuries. This rich and flavorful cuisine is a favorite among foodies and home cooks alike.\n",
      "\n",
      "The heart of Bolognese cuisine is the ragù, a slow-cooked sauce made with ground beef, pork, and vegetables. This sauce is the base for many of the dishes in the region, including the famous tagliatelle al ragù. This dish is made with fresh egg pasta, the ragù, and Parmigiano-Reggiano cheese.\n",
      "\n",
      "Another classic Bolognese dish is tortellini in brodo. This dish is made with small pasta rings filled with a mixture of pork, mortadella, and Parmigiano-Reggiano cheese, served in a flavorful broth.\n",
      "\n",
      "Bolognese cuisine also features a variety of delicious desserts. The most famous of these is the zuppa inglese, a layered cake made with sponge cake, custard, and liqueur.\n",
      "\n",
      "Bolognese cuisine is a delicious and varied cuisine that is sure to please any palate. From the classic ragù to the decadent zuppa inglese, Bolognese cuisine is sure to delight and satisfy.\n"
     ]
    }
   ],
   "source": [
    "# Extract and print the generated text\n",
    "generated_text = response.choices[0].text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa30df54",
   "metadata": {},
   "source": [
    "### Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba9a0e5",
   "metadata": {},
   "source": [
    "OpenAI.Completion is an endpoint of the OpenAI API that allows you to interact with GPT language models (text-davinci, davinci, curie, babbage, ada) to generate human-like text completions based on a given prompt.\n",
    "\n",
    "These models have been trained on vast amounts of text data and are capable of understanding context and generating coherent responses across various domains.\n",
    "\n",
    "The OpenAI.Completion API endpoint is located at: https://api.openai.com/v1/completions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4c197",
   "metadata": {},
   "source": [
    "The create() method from the openai.Completion module allows you to perform completion. The parameter values are as follows:\n",
    "\n",
    "- **Input Prompt**: You provide a text prompt or context as input to the GPT-3.5 model. This prompt can be as short as a sentence or as long as a paragraph, depending on your requirements. The prompt sets the context for the model to generate text.\n",
    "\n",
    "- **API Request**: You make an API request to the OpenAI API, specifying the model, es. davinci engine (which uses the GPT-3.5 model), and the completions endpoint. You pass the input prompt as part of the request.\n",
    "\n",
    "- **Model Processing**: The GPT-3.5 model processes the input prompt and generates a continuation of the text based on the context provided. It's capable of understanding and completing text in a coherent and contextually relevant manner.\n",
    "\n",
    "- **Response**: The API returns a response that includes the text completion generated by the model. You can extract this generated text from the response and use it in your application.\n",
    "\n",
    "You can adjust the **temperature** parameter when making an API request. A higher temperature value (e.g., 0.8) makes the output more random and creative, while a lower value (e.g., 0.2) makes it more deterministic and focused."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f5d4eb",
   "metadata": {},
   "source": [
    "### Chat Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32713ed9",
   "metadata": {},
   "source": [
    "OpenAI.Chat.Completion is a new OpenAI API endpoint for interacting with the latest and most capable language models (gpt-4 and gpt-3.5-turbo) to generate human-like text completions based on a dialogue. These models have been trained on vast amounts of text data and are capable of understanding context and generating coherent responses across various domains. You can perform all the tasks that can be performed by the Completions endpoint, including text completion, text generation, language translation, sentiment analysis, text classification, code generation, summarization, text insertion and text to emojis.\n",
    "\n",
    "The OpenAI.Chat.Completion API endpoint is located at: https://api.openai.com/v1/chat/completions.\n",
    "\n",
    "What’s the difference between Completions and Chat Completions APIs? The Completions endpoint (v1/Completions) completes a single prompt and takes a single text as input, while the Chat Completions endpoint (v1/Chat/Completions) responds to a specified dialogue and requires input in a specific format with the message history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf0836",
   "metadata": {},
   "source": [
    "Unlike the ChatGPT website, which saves and remembers your messages to make a dialog, the API does not keep prior messages. As a result, you will need to adopt a new prompt style to have an interactive and dynamic conversation, as detailed below:\n",
    "\n",
    "Instead of sending a single string as your prompt, you have to send a list of messages as your input.\n",
    "Each message in the list has two properties: role and content\n",
    "The role parameter is used to specify the role or identity of the message sender, it can takes three values:\n",
    "- ‘user’ : it means you, or who is chatting, or who is asking to the model.\n",
    "- ‘assistant’ : it means the model that is replying to the user’s questions.\n",
    "- ‘system’: it means the system, and it’s used to set the context, give instructions to the assistant, or guide the conversation.\n",
    "The content parameter is where the message of the corresponding role is provided. The content can include questions, statements, commands, or any other text-based input relevant to the conversation.\n",
    "The messages are processed in the order they appear in the list, and the assistant responds accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b328d",
   "metadata": {},
   "source": [
    "The following code demonstrates how to use OpenAI's ChatGPT API to engage in a conversation with the model and obtain responses. It involves a series of messages exchanged between the user and the model. Here's a step-by-step explanation of what the code does:\n",
    "\n",
    "1. `completion = openai.ChatCompletion.create(`:\n",
    "   - This line sends a request to the OpenAI ChatGPT API to create a chat-based completion. It includes various parameters and data to specify the conversation and obtain a response.\n",
    "   \n",
    "2. Parameters in the request:\n",
    "   - `model = \"gpt-3.5-turbo\"`: Specifies the model to use for chat-based completion. In this case, \"gpt-3.5-turbo\" is selected, which is optimized for conversation-style interactions.\n",
    "\n",
    "   - `temperature = 0.8`: The `temperature` parameter controls the level of randomness in the generated text. A value of 0.8 allows for moderate randomness, resulting in more varied and creative responses.\n",
    "\n",
    "   - `max_tokens = 2000`: The `max_tokens` parameter sets an upper limit on the length of the generated text. In this case, it's set to 2000 tokens, ensuring that the response doesn't exceed this length.\n",
    "\n",
    "   - `messages`: This parameter contains a list of message objects. Each message has a \"role\" and \"content.\" The conversation unfolds as follows:\n",
    "     - A system message: \"You are a funny comedian who tells dad jokes.\" This message sets the model's role and context for the conversation.\n",
    "     - A user message: \"Write a dad joke related to numbers.\" This is the user's request.\n",
    "     - An assistant message: \"Q: How do you make 7 even? A: Take away the s.\" This is the assistant's response.\n",
    "     - Another user message: \"Write one related to programmers.\" This is a follow-up request from the user.\n",
    "\n",
    "3. `print(completion.choices[0].message)`: After the API request is made and a response is received, this line extracts and prints the assistant's message (response) from the `completion` object.\n",
    "\n",
    "In summary, the code engages in a chat-based interaction with the model using the ChatGPT API. The conversation consists of user requests and model responses. The response from the assistant is printed, allowing you to see how the model continues the conversation based on the context provided in the messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e9806",
   "metadata": {},
   "source": [
    "> **Note - Messages** are arrays of message objects. Each object has a `role` (system, user, assistant) and a `content`, the message as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "747c52f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": \"Q: Why do programmers always mix up Christmas and Halloween? A: Because Oct 31 == Dec 25.\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "     model = \"gpt-3.5-turbo\"\n",
    "   , temperature = 0.8\n",
    "   , max_tokens  = 2000\n",
    "   , messages = [ {\"role\": \"system\", \"content\": \"You are a funny comedian who tells dad jokes.\"}\n",
    "                 ,{\"role\": \"user\", \"content\": \"Write a dad joke related to numbers.\"}\n",
    "                 ,{\"role\": \"assistant\", \"content\": \"Q: How do you make 7 even? A: Take away the s.\"}\n",
    "                 ,{\"role\": \"user\", \"content\": \"Write one related to programmers.\"}])\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ef8a9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"Q: Why do programmers always mix up Christmas and Halloween? A: Because Oct 31 == Dec 25.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1696426228,\n",
      "  \"id\": \"chatcmpl-85wFI8t9YhgsGyeuS6btMpWA16GT0\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 23,\n",
      "    \"prompt_tokens\": 60,\n",
      "    \"total_tokens\": 83\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b9a2f0",
   "metadata": {},
   "source": [
    "## Parse Unstructured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52d2b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"You will be provided with unstructured data, and your task is to parse it into CSV format using ';' as a separator\"\n",
    "data   = '''\n",
    "There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, \n",
    "which are purple and taste like candy. There are also loheckles, which are a grayish blue fruit and are very tart, a \n",
    "little bit like a lemon. Pounits are a bright green color and are more savory than sweet. There are also plenty of \n",
    "loopnovas which are a neon pink flavor and taste like cotton candy. Finally, there are fruits called glowls, which \n",
    "have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a1b1b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": \"Fruit Name;Color;Taste\\nneoskizzles;purple;candy\\nloheckles;grayish blue;tart\\npounits;bright green;savory\\nloopnovas;neon pink;cotton candy\\nglowls;pale orange;sour, bitter, acidic, caustic\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "     model = \"gpt-3.5-turbo\"\n",
    "   , temperature = 0.8\n",
    "   , max_tokens  = 2000\n",
    "   , messages = [ {\"role\": \"system\", \"content\": prompt}\n",
    "                 ,{\"role\": \"user\"  , \"content\": data}\n",
    "                ])\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bae94dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table>\n",
      "  <tr>\n",
      "    <th>Fruit</th>\n",
      "    <th>Color</th>\n",
      "    <th>Taste</th>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td>Neoskizzles</td>\n",
      "    <td>Purple</td>\n",
      "    <td>Candy-like</td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td>Loheckles</td>\n",
      "    <td>Grayish blue</td>\n",
      "    <td>Tart</td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td>Pounits</td>\n",
      "    <td>Bright green</td>\n",
      "    <td>Savory</td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td>Loopnovas</td>\n",
      "    <td>Neon pink</td>\n",
      "    <td>Cotton candy</td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td>Glowls</td>\n",
      "    <td>Pale orange</td>\n",
      "    <td>Sour and bitter</td>\n",
      "  </tr>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You will be provided with unstructured data, and your task is to parse it into HTML table format\"\n",
    "completion = openai.ChatCompletion.create(\n",
    "     model = \"gpt-3.5-turbo\"\n",
    "   , temperature = 0.8\n",
    "   , max_tokens  = 2000\n",
    "   , messages = [ {\"role\": \"system\", \"content\": prompt}\n",
    "                 ,{\"role\": \"user\"  , \"content\": data}\n",
    "                ])\n",
    "\n",
    "response = completion.choices[0].message \n",
    "print(response[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d08522ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81cb47e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "  <tr>\n",
       "    <th>Fruit</th>\n",
       "    <th>Color</th>\n",
       "    <th>Taste</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Neoskizzles</td>\n",
       "    <td>Purple</td>\n",
       "    <td>Candy-like</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Loheckles</td>\n",
       "    <td>Grayish blue</td>\n",
       "    <td>Tart</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Pounits</td>\n",
       "    <td>Bright green</td>\n",
       "    <td>Savory</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Loopnovas</td>\n",
       "    <td>Neon pink</td>\n",
       "    <td>Cotton candy</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Glowls</td>\n",
       "    <td>Pale orange</td>\n",
       "    <td>Sour and bitter</td>\n",
       "  </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(response[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d35ea",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4ec9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt   = \"You will be provided with a sentence in English, and your task is to translate it into Italian.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70509291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ci sono molti frutti che sono stati trovati sul pianeta Goocrux, recentemente scoperto. Ci sono neoskizzles che crescono lì, che sono viola e hanno un sapore simile ai caramelle. Ci sono anche loheckles, che sono un frutto grigio-azzurro e sono molto acidi, un po' come un limone. I pounits sono di un colore verde brillante e sono più saporiti che dolci. Ci sono anche molte loopnovas, che sono di un colore rosa neon e hanno un sapore di zucchero filato. Infine, ci sono dei frutti chiamati glowls, che hanno un sapore molto acido e amaro, che è acido e corrosivo, e un leggero tono arancione.\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "     model = \"gpt-3.5-turbo\"\n",
    "   , temperature = 0.8\n",
    "   , max_tokens  = 2000\n",
    "   , messages = [ {\"role\": \"system\", \"content\": prompt}\n",
    "                 ,{\"role\": \"user\"  , \"content\": data}\n",
    "                ])\n",
    "\n",
    "response = completion.choices[0].message\n",
    "response = response[\"content\"]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f183a1cf",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a05b35ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In short, a neural network can either refer to a network of biological neurons or a network of artificial neurons. Artificial neural networks are used to solve AI problems by modeling connections between nodes as weights, and a linear combination and activation function is used to create an output within a specified range. They can be trained and can self-learn from experience.\n"
     ]
    }
   ],
   "source": [
    "# Define a long document to be summarized\n",
    "document = \"\"\"\n",
    "A neural network can refer to either a neural circuit of biological neurons (sometimes also called a biological neural network),\n",
    "or a network of artificial neurons or nodes in the case of an artificial neural network. Artificial neural networks are used \n",
    "for solving artificial intelligence (AI) problems; they model connections of biological neurons as weights between nodes. \n",
    "A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are \n",
    "modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function \n",
    "controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could \n",
    "be −1 and 1. These artificial networks may be used for predictive modeling, adaptive control and applications where they \n",
    "can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions \n",
    "from a complex and seemingly unrelated set of information\n",
    "\"\"\"\n",
    "\n",
    "# Summarize the document\n",
    "response = openai.Completion.create(\n",
    "    engine=\"text-davinci-003\",\n",
    "    prompt=f\"Summarize the following document:\\n{document}\",\n",
    "    max_tokens=100  # Adjust as needed\n",
    ")\n",
    "\n",
    "# Extract and print the summary\n",
    "summary = response.choices[0].text\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db5bd28",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa007b80",
   "metadata": {},
   "source": [
    "Let’s define a function that takes a text input and returns the sentiment analysis result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87e38f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-002\",\n",
    "        prompt=f\"Sentiment analysis of the following text:\\n{text}\\n\",\n",
    "        temperature=0,\n",
    "    )\n",
    "    sentiment = response.choices[0].text.strip()\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e2a0f",
   "metadata": {},
   "source": [
    "In this function, we are using the `Completion.create` method of the OpenAI API to get the sentiment analysis result. We are using the `text-davinci-002` engine for this task. As usual, the prompt parameter is used to provide the input text to the model, and the other parameters are used to control the behavior of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "511f3947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the text is negative.\n"
     ]
    }
   ],
   "source": [
    "text = \"The film I saw yesterday was awesome!\"\n",
    "text = \"This item doesn't work at all for me!\"\n",
    "sentiment = get_sentiment(text)\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca93b13",
   "metadata": {},
   "source": [
    "## Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2bd7980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "def factorial(n): \n",
      "  \n",
      "    # Base Case - 1! = 1\n",
      "    if n == 1: \n",
      "        return 1\n",
      "  \n",
      "    # Recursive Case\n",
      "    else: \n",
      "        return n * factorial(n-1)\n"
     ]
    }
   ],
   "source": [
    "# Define a problem statement for code generation\n",
    "problem_statement = \"Create a Python function that calculates the factorial of a number.\"\n",
    "#problem_statement += \"Please insert also a comment to explain the code\"\n",
    "\n",
    "# Generate Python code\n",
    "response = openai.Completion.create(\n",
    "    engine=\"text-davinci-002\",\n",
    "    prompt=f\"Generate Python code for the following task:\\n{problem_statement}\",\n",
    "    max_tokens=100  # Adjust as needed\n",
    ")\n",
    "\n",
    "# Extract and print the generated code\n",
    "generated_code = response.choices[0].text\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad1f9a4",
   "metadata": {},
   "source": [
    "## Reference and Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b509b",
   "metadata": {},
   "source": [
    "Documentation from the official site of OpenAI API"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": "9",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
